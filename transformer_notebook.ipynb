{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYy22wYG5Cpu",
        "outputId": "6531c26f-9492-42da-fcab-418bd93282ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.1-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2024.11.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sacremoses) (4.67.1)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.8.1-py3-none-any.whl (982 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.0/983.0 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.15.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 sacremoses-0.1.1 torchmetrics-1.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sacremoses torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-08T13:30:19.785643Z",
          "iopub.status.busy": "2025-08-08T13:30:19.784975Z",
          "iopub.status.idle": "2025-08-08T13:30:19.789877Z",
          "shell.execute_reply": "2025-08-08T13:30:19.789127Z",
          "shell.execute_reply.started": "2025-08-08T13:30:19.785618Z"
        },
        "id": "nwKFh_PV5A5j",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# Huggingface datasets and tokenizers\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torchmetrics\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-08T13:30:19.903171Z",
          "iopub.status.busy": "2025-08-08T13:30:19.902956Z",
          "iopub.status.idle": "2025-08-08T13:30:19.920365Z",
          "shell.execute_reply": "2025-08-08T13:30:19.919775Z",
          "shell.execute_reply.started": "2025-08-08T13:30:19.903154Z"
        },
        "id": "nGAC60AY5A5p",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Transformer model parameters\n",
        "MODEL_NUMBER_OF_LAYERS = 3              # paper value : 6\n",
        "MODEL_DIMENSION = 256                   # 512\n",
        "MODEL_NUMBER_OF_HEADS = 4               # 8\n",
        "MODEL_INNER_LAYER_DIMENSION = 1024      # 2048\n",
        "MODEL_DROPOUT_PROBABILITY = 0.1         # 0.1\n",
        "MODEL_LABEL_SMOOTHING_VALUE = 0.1       # 0.1\n",
        "SEQUENCE_LENGTH = 500\n",
        "\n",
        "# Training parameters\n",
        "BATCH_SIZE = 8\n",
        "NUMBER_OF_EPOCHS = 15\n",
        "BETA1 = 0.9                             # 0.9\n",
        "BETA2 = 0.98                            # 0.98\n",
        "EPSILON = 1e-9                          # 1e-9\n",
        "WARMUP_STEPS = 4000                     # 4000\n",
        "\n",
        "# Dataset parameters\n",
        "DATASET_NAME = \"Helsinki-NLP/opus_books\"\n",
        "SOURCE_LANGUAGE = \"en\"\n",
        "TARGET_LANGUAGE = \"fr\"\n",
        "\n",
        "# Saving parameters\n",
        "MODEL_FOLDER = \"weights\"\n",
        "MODEL_BASENAME = \"tmodel_\"\n",
        "MODEL_PRELOAD = \"latest\"\n",
        "EXPERIMENT_FOLDER = \"runs/tmodel\"\n",
        "\n",
        "# Special tokens\n",
        "UNK_TOKEN = '[UNK]'\n",
        "SOS_TOKEN = '[SOS]'\n",
        "EOS_TOKEN = '[EOS]'\n",
        "PAD_TOKEN = \"[PAD]\"\n",
        "\n",
        "CHECKPOINTS_PATH = os.path.join(os.getcwd(), 'models', 'checkpoints') # semi-trained models during training will be dumped here\n",
        "BINARIES_PATH = os.path.join(os.getcwd(), 'models', 'binaries') # location where trained models are located\n",
        "DATA_DIR_PATH = os.path.join(os.getcwd(), 'data') # training data will be stored here\n",
        "\n",
        "os.makedirs(CHECKPOINTS_PATH, exist_ok=True)\n",
        "os.makedirs(BINARIES_PATH, exist_ok=True)\n",
        "os.makedirs(DATA_DIR_PATH, exist_ok=True)\n",
        "\n",
        "def get_weights_file_path(epoch: str):\n",
        "    model_folder = f\"{DATASET_NAME}_{MODEL_FOLDER}\"\n",
        "    model_filename = f\"{MODEL_BASENAME}{epoch}.pt\"\n",
        "    return str(Path('.') / model_folder / model_filename)\n",
        "\n",
        "# Find the latest weights file in the weights folder\n",
        "def latest_weights_file_path():\n",
        "    model_folder = f\"{DATASET_NAME}_{MODEL_FOLDER}\"\n",
        "    model_filename = f\"{MODEL_BASENAME}*\"\n",
        "    weights_files = list(Path(model_folder).glob(model_filename))\n",
        "    if len(weights_files) == 0:\n",
        "        return None\n",
        "    weights_files.sort()\n",
        "    return str(weights_files[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-08-08T13:30:19.876202Z",
          "iopub.status.busy": "2025-08-08T13:30:19.875942Z",
          "iopub.status.idle": "2025-08-08T13:30:19.901792Z",
          "shell.execute_reply": "2025-08-08T13:30:19.901098Z",
          "shell.execute_reply.started": "2025-08-08T13:30:19.876184Z"
        },
        "id": "mPgmPAI45A5n",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocabulary_size, model_dimension):\n",
        "        super().__init__()\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        self.model_dimension = model_dimension\n",
        "        self.embedding = nn.Embedding(vocabulary_size, model_dimension)\n",
        "\n",
        "    def forward(self, src_token_ids):\n",
        "        return self.embedding(src_token_ids) * math.sqrt(self.model_dimension)\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, model_dimension, dropout_probability, max_sequence_length=5000):\n",
        "        super().__init__()\n",
        "        self.sequence_length = max_sequence_length\n",
        "        self.model_dimension = model_dimension\n",
        "        self.dropout = nn.Dropout(dropout_probability)\n",
        "\n",
        "        positional_encoding = torch.zeros(max_sequence_length, model_dimension)\n",
        "\n",
        "        positions = torch.arange(0, max_sequence_length, 1, dtype=float)\n",
        "        positions = torch.unsqueeze(positions, 1)\n",
        "        denominator = torch.exp(torch.arange(0, model_dimension, 2, dtype=float) * -math.log(10000.) / model_dimension)\n",
        "\n",
        "        positional_encoding[:,0::2] = torch.sin(positions * denominator)\n",
        "        positional_encoding[:,1::2] = torch.cos(positions * denominator)\n",
        "\n",
        "        positional_encoding = positional_encoding.unsqueeze(0)\n",
        "        self.register_buffer(\"positional_encoding\", positional_encoding)\n",
        "\n",
        "    def forward(self, src_embedded):\n",
        "        return self.dropout(src_embedded + self.positional_encoding[:, :src_embedded.shape[1]])\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, key_dimension, value_dimension):\n",
        "        super().__init__()\n",
        "        self.key_dimension = key_dimension\n",
        "        self.value_dimension = value_dimension\n",
        "\n",
        "    def forward(self, queries, keys, values, mask=None):\n",
        "        dot_product = torch.matmul(queries, keys.transpose(-2, -1))\n",
        "        dot_product /= math.sqrt(self.key_dimension)\n",
        "\n",
        "        if mask is not None:\n",
        "            dot_product.masked_fill_(mask == 0, float(\"-inf\"))\n",
        "\n",
        "        weights = dot_product.softmax(dim=-1)\n",
        "\n",
        "        return torch.matmul(weights, values), weights\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, model_dimension, number_of_heads, save_weigths=False):\n",
        "        super().__init__()\n",
        "        self.model_dimension = model_dimension\n",
        "        self.number_of_heads = number_of_heads\n",
        "        self.key_dimension = model_dimension // number_of_heads\n",
        "        self.value_dimension = model_dimension // number_of_heads\n",
        "\n",
        "        self.linear_queries = nn.Linear(model_dimension, model_dimension) # W_Q\n",
        "        self.linear_keys = nn.Linear(model_dimension, model_dimension) # W_K\n",
        "        self.linear_values = nn.Linear(model_dimension, model_dimension) # W_V\n",
        "\n",
        "        self.linear_output = nn.Linear(model_dimension, model_dimension) # w_0\n",
        "\n",
        "        self.attention_weigths = None\n",
        "        self.save_weigths = save_weigths\n",
        "\n",
        "    def forward(self, queries, keys, values, mask=None):\n",
        "        projected_queries = self.linear_queries(queries) # Q * W_Q\n",
        "        projected_queries = projected_queries.view(projected_queries.shape[0], projected_queries.shape[1], self.number_of_heads, self.key_dimension).transpose(1, 2)\n",
        "\n",
        "        projected_keys = self.linear_keys(keys) # K * W_K\n",
        "        projected_keys = projected_keys.view(projected_keys.shape[0], projected_keys.shape[1], self.number_of_heads, self.key_dimension).transpose(1, 2)\n",
        "\n",
        "        projected_values = self.linear_values(values) # V * W_V\n",
        "        projected_values = projected_values.view(projected_values.shape[0], projected_values.shape[1], self.number_of_heads, self.value_dimension).transpose(1, 2)\n",
        "\n",
        "        scaled_dot_product = ScaledDotProductAttention(self.key_dimension, self.value_dimension)\n",
        "\n",
        "        attention, attention_weigths = scaled_dot_product(projected_queries, projected_keys, projected_values, mask)\n",
        "        attention = attention.transpose(1, 2).contiguous()\n",
        "        attention = attention.view(queries.shape[0], -1, self.number_of_heads * self.key_dimension)\n",
        "\n",
        "        if self.save_weigths :\n",
        "            self.attention_weigths = attention_weigths.detach()\n",
        "\n",
        "        return self.linear_output(attention)\n",
        "\n",
        "\n",
        "class PositionwiseFeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, model_dimension, inner_layer_dimension):\n",
        "        super().__init__()\n",
        "        self.modul_dimension = model_dimension\n",
        "        self.inner_layer_dimension = inner_layer_dimension\n",
        "\n",
        "        self.linear1 = nn.Linear(model_dimension, inner_layer_dimension) # W1 and b1\n",
        "        self.linear2 = nn.Linear(inner_layer_dimension, model_dimension) # W2 and b2\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        return self.linear2(x)\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, model_dimension, multihead_attention, feedforward_network, dropout_probability):\n",
        "        super().__init__()\n",
        "        self.model_dimension = model_dimension\n",
        "\n",
        "        self.multihead_attention = multihead_attention\n",
        "        self.feedforward_network = feedforward_network\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(model_dimension)\n",
        "        self.layernorm2 = nn.LayerNorm(model_dimension)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_probability)\n",
        "\n",
        "    def forward(self, src_embedded, src_mask=None):\n",
        "        attention_ouput = self.multihead_attention(src_embedded, src_embedded, src_embedded, src_mask)\n",
        "        sublayer_output1 = self.layernorm1(src_embedded + self.dropout(attention_ouput))\n",
        "\n",
        "        ffnetwork_output = self.feedforward_network(sublayer_output1)\n",
        "        sublayer_output2 = self.layernorm2(sublayer_output1 + self.dropout(ffnetwork_output))\n",
        "\n",
        "        return sublayer_output2\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, model_dimension, number_of_layers, number_of_heads, inner_layer_dimension, dropout_probability):\n",
        "        super().__init__()\n",
        "        self.model_dimension = model_dimension\n",
        "        self.number_of_layers = number_of_layers\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(\n",
        "                model_dimension,\n",
        "                MultiHeadAttention(model_dimension, number_of_heads),\n",
        "                PositionwiseFeedForwardNetwork(model_dimension, inner_layer_dimension),\n",
        "                dropout_probability\n",
        "            )\n",
        "            for _ in range(number_of_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, src_embedded, src_mask=None):\n",
        "        src_encoder_output = src_embedded\n",
        "\n",
        "        for layer in self.layers:\n",
        "            src_encoder_output = layer(src_encoder_output, src_mask)\n",
        "\n",
        "        return src_encoder_output\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, model_dimension, masked_multihead_attention, multihead_attention, feedforward_network, dropout_probability):\n",
        "        super().__init__()\n",
        "        self.model_dimension = model_dimension\n",
        "\n",
        "        self.masked_multihead_attention = masked_multihead_attention\n",
        "        self.multihead_attention = multihead_attention\n",
        "        self.feedforward_network = feedforward_network\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(model_dimension)\n",
        "        self.layernorm2 = nn.LayerNorm(model_dimension)\n",
        "        self.layernorm3 = nn.LayerNorm(model_dimension)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_probability)\n",
        "\n",
        "    def forward(self, src_encoder_output, trg_embedded, src_mask=None, trg_mask=None):\n",
        "        masked_attention_output = self.masked_multihead_attention(queries=trg_embedded, keys=trg_embedded, values=trg_embedded, mask=trg_mask)\n",
        "        sublayer_output1 = self.layernorm1(trg_embedded + self.dropout(masked_attention_output))\n",
        "\n",
        "        attention_ouput = self.multihead_attention(queries=sublayer_output1, keys=src_encoder_output, values=src_encoder_output, mask=src_mask)\n",
        "        sublayer_output2 = self.layernorm2(sublayer_output1 + self.dropout(attention_ouput))\n",
        "\n",
        "        ffnetwork_ouput = self.feedforward_network(sublayer_output2)\n",
        "        sublayer_output3 = self.layernorm3(sublayer_output2 + self.dropout(ffnetwork_ouput))\n",
        "\n",
        "        return sublayer_output3\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, model_dimension, number_of_layers, number_of_heads, inner_layer_dimension, dropout_probability):\n",
        "        super().__init__()\n",
        "        self.model_dimension = model_dimension\n",
        "        self.number_of_layers = number_of_layers\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(\n",
        "                model_dimension,\n",
        "                MultiHeadAttention(model_dimension, number_of_heads),  # masked self-attention\n",
        "                MultiHeadAttention(model_dimension, number_of_heads),  # encoder-decoder attention\n",
        "                PositionwiseFeedForwardNetwork(model_dimension, inner_layer_dimension),\n",
        "                dropout_probability\n",
        "            )\n",
        "            for _ in range(number_of_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, src_encoder_output, trg_embedded, src_mask=None, trg_mask=None):\n",
        "        trg_decoder_output = trg_embedded\n",
        "\n",
        "        for layer in self.layers:\n",
        "            trg_decoder_output = layer(src_encoder_output, trg_decoder_output, src_mask=src_mask, trg_mask=trg_mask)\n",
        "\n",
        "        return trg_decoder_output\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, model_dimension, inner_layer_dimension, number_of_layers, number_of_heads, src_vocabulary_size, trg_vocabulary_size, dropout_probability):\n",
        "        super().__init__()\n",
        "        self.model_dimension = model_dimension\n",
        "        self.number_of_layers = number_of_layers\n",
        "        self.number_of_heads = number_of_heads\n",
        "        self.src_vocabulary_size = src_vocabulary_size\n",
        "        self.trg_vocabulary_size = trg_vocabulary_size\n",
        "\n",
        "        self.input_embedding = Embedding(src_vocabulary_size, model_dimension)\n",
        "        self.output_embedding = Embedding(trg_vocabulary_size, model_dimension)\n",
        "\n",
        "        self.input_pos_encoding = PositionalEncoding(model_dimension, dropout_probability)\n",
        "        self.output_pos_encoding = PositionalEncoding(model_dimension, dropout_probability)\n",
        "\n",
        "        self.encoder = Encoder(model_dimension, number_of_layers, number_of_heads, inner_layer_dimension, dropout_probability)\n",
        "        self.decoder = Decoder(model_dimension, number_of_layers, number_of_heads, inner_layer_dimension, dropout_probability)\n",
        "\n",
        "        self.linear_projection = nn.Linear(model_dimension, trg_vocabulary_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def encode(self, src_token_ids, src_mask=None):\n",
        "        input_embedded = self.input_embedding(src_token_ids)\n",
        "        input_pos_encoded = self.input_pos_encoding(input_embedded)\n",
        "        input_encoded = self.encoder(input_pos_encoded, src_mask)\n",
        "        return input_encoded\n",
        "\n",
        "    def decode(self, input_encoded, trg_token_ids, src_mask, trg_mask=None):\n",
        "        output_embedded = self.output_embedding(trg_token_ids)\n",
        "        output_pos_encoded = self.output_pos_encoding(output_embedded)\n",
        "        output_decoded = self.decoder(input_encoded, output_pos_encoded, src_mask, trg_mask)\n",
        "        return output_decoded\n",
        "\n",
        "    def forward(self, src_token_ids, trg_token_ids, src_mask=None, trg_mask=None):\n",
        "        input_embedded = self.input_embedding(src_token_ids)\n",
        "        input_pos_encoded = self.input_pos_encoding(input_embedded)\n",
        "        input_encoded = self.encoder(input_pos_encoded, src_mask)\n",
        "\n",
        "        output_embedded = self.output_embedding(trg_token_ids)\n",
        "        output_pos_encoded = self.output_pos_encoding(output_embedded)\n",
        "        output_decoded = self.decoder(input_encoded, output_pos_encoded, src_mask, trg_mask)\n",
        "\n",
        "        output_decoded = self.linear_projection(output_decoded)\n",
        "        # output_decoded = output_decoded.softmax(dim=-1)\n",
        "\n",
        "        return output_decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-08T13:30:19.921430Z",
          "iopub.status.busy": "2025-08-08T13:30:19.921178Z",
          "iopub.status.idle": "2025-08-08T13:30:19.940233Z",
          "shell.execute_reply": "2025-08-08T13:30:19.939577Z",
          "shell.execute_reply.started": "2025-08-08T13:30:19.921408Z"
        },
        "id": "iDFokEvf5A5u",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class EnglishToFrenchDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, src_tokenizer, trg_tokenizer, src_language, trg_language, sequence_length):\n",
        "        super().__init__()\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.trg_tokenizer = trg_tokenizer\n",
        "        self.src_language = src_language\n",
        "        self.trg_language = trg_language\n",
        "\n",
        "        self.sos_token = torch.tensor([trg_tokenizer.token_to_id(SOS_TOKEN)], dtype=torch.int64)\n",
        "        self.eos_token = torch.tensor([trg_tokenizer.token_to_id(EOS_TOKEN)], dtype=torch.int64)\n",
        "        self.pad_token = torch.tensor([trg_tokenizer.token_to_id(PAD_TOKEN)], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_target_pair = self.dataset[idx]\n",
        "        src_text = src_target_pair['translation'][self.src_language]\n",
        "        trg_text = src_target_pair['translation'][self.trg_language]\n",
        "\n",
        "        # Transform the text into tokens\n",
        "        enc_input_tokens = self.src_tokenizer.encode(src_text).ids\n",
        "        dec_input_tokens = self.trg_tokenizer.encode(trg_text).ids\n",
        "\n",
        "        max_token_length = self.sequence_length - 2\n",
        "        if len(enc_input_tokens) > max_token_length:\n",
        "            enc_input_tokens = enc_input_tokens[:max_token_length]\n",
        "        if len(dec_input_tokens) > max_token_length - 1:  # -1 car decoder a seulement SOS au début\n",
        "            dec_input_tokens = dec_input_tokens[:max_token_length - 1]\n",
        "\n",
        "        # Add sos, eos and padding to each sentence\n",
        "        enc_num_padding_tokens = self.sequence_length - len(enc_input_tokens) - 2  # We will add <s> and </s>\n",
        "        # We will only add <s>, and </s> only on the label\n",
        "        dec_num_padding_tokens = self.sequence_length - len(dec_input_tokens) - 1\n",
        "\n",
        "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
        "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
        "            raise ValueError(\"Sentence is too long\")\n",
        "\n",
        "        # Add <s> and </s> token\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Add only <s> token\n",
        "        decoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Add only </s> token\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Double check the size of the tensors to make sure they are all sequence_length long\n",
        "        assert encoder_input.size(0) == self.sequence_length\n",
        "        assert decoder_input.size(0) == self.sequence_length\n",
        "        assert label.size(0) == self.sequence_length\n",
        "\n",
        "        encoder_mask = (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0)\n",
        "        decoder_mask = (decoder_input != self.pad_token).unsqueeze(0) & causal_mask(decoder_input.size(0))\n",
        "\n",
        "        return {\n",
        "            \"encoder_input\": encoder_input,  # (sequence_length)\n",
        "            \"decoder_input\": decoder_input,  # (sequence_length)\n",
        "            \"encoder_mask\": encoder_mask, # (1, 1, sequence_length)\n",
        "            \"decoder_mask\": decoder_mask, # (1, sequence_length) & (1, sequence_length, sequence_length),\n",
        "            \"label\": label,  # (sequence_length)\n",
        "            \"src_text\": src_text,\n",
        "            \"trg_text\": trg_text,\n",
        "        }\n",
        "\n",
        "def causal_mask(size):\n",
        "    mask = torch.triu(torch.ones((size, size)), diagonal=1).type(torch.int)\n",
        "    return (mask == 0).unsqueeze(0)\n",
        "\n",
        "\n",
        "def get_all_sentences(dataset, language):\n",
        "    for item in dataset:\n",
        "        yield item[\"translation\"][language]\n",
        "\n",
        "\n",
        "def get_tokenizer(dataset, language):\n",
        "    tokenizer_path = Path(\"tokenizer_{0}.json\".format(language))\n",
        "    if not Path.exists(tokenizer_path):\n",
        "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token=UNK_TOKEN))\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = WordLevelTrainer(special_tokens=[UNK_TOKEN, PAD_TOKEN, SOS_TOKEN, EOS_TOKEN], min_frequency=2)\n",
        "        tokenizer.train_from_iterator(get_all_sentences(dataset, language), trainer=trainer)\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "def get_dataset():\n",
        "    dataset = load_dataset(DATASET_NAME, SOURCE_LANGUAGE + \"-\" + TARGET_LANGUAGE, split=\"train\")\n",
        "    dataset = dataset.shuffle().select(range(int(len(dataset)/4)))\n",
        "\n",
        "    src_tokenizer = get_tokenizer(dataset, SOURCE_LANGUAGE)\n",
        "    trg_tokenizer = get_tokenizer(dataset, TARGET_LANGUAGE)\n",
        "\n",
        "    max_length = SEQUENCE_LENGTH - 2\n",
        "\n",
        "    def is_valid_length(example):\n",
        "        src_tokens = src_tokenizer.encode(example['translation'][SOURCE_LANGUAGE]).ids\n",
        "        trg_tokens = trg_tokenizer.encode(example['translation'][TARGET_LANGUAGE]).ids\n",
        "        return len(src_tokens) <= max_length and len(trg_tokens) <= max_length\n",
        "\n",
        "    dataset = dataset.filter(is_valid_length)\n",
        "    print(f\"Dataset filtred: {len(dataset)} examples left\")\n",
        "\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    validation_size = len(dataset) - train_size\n",
        "\n",
        "    train_dataset, valisation_dataset = random_split(dataset, [train_size, validation_size])\n",
        "\n",
        "    train_dataset = EnglishToFrenchDataset(train_dataset, src_tokenizer, trg_tokenizer, SOURCE_LANGUAGE, TARGET_LANGUAGE, SEQUENCE_LENGTH)\n",
        "    validation_dataset = EnglishToFrenchDataset(valisation_dataset, src_tokenizer, trg_tokenizer, SOURCE_LANGUAGE, TARGET_LANGUAGE, SEQUENCE_LENGTH)\n",
        "\n",
        "    # Find the maximum length of each sentence in the source and target sentence\n",
        "    src_max_length = 0\n",
        "    trg_max_length = 0\n",
        "\n",
        "    for item in dataset:\n",
        "        src_ids = src_tokenizer.encode(item['translation'][SOURCE_LANGUAGE]).ids\n",
        "        trg_ids = trg_tokenizer.encode(item['translation'][TARGET_LANGUAGE]).ids\n",
        "        src_max_length = max(src_max_length, len(src_ids))\n",
        "        trg_max_length = max(trg_max_length, len(trg_ids))\n",
        "\n",
        "    print(f'Max length of source sentence: {src_max_length}')\n",
        "    print(f'Max length of target sentence: {trg_max_length}')\n",
        "\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=True) # Process the sentences one by one\n",
        "\n",
        "    return train_dataloader, val_dataloader, src_tokenizer, trg_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-08T13:30:19.941110Z",
          "iopub.status.busy": "2025-08-08T13:30:19.940915Z",
          "iopub.status.idle": "2025-08-08T13:30:19.960133Z",
          "shell.execute_reply": "2025-08-08T13:30:19.959450Z",
          "shell.execute_reply.started": "2025-08-08T13:30:19.941095Z"
        },
        "id": "E54tYDlh5A5w",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_transformer(src_vocab_size, trg_vocab_size):\n",
        "    transformer = Transformer(model_dimension=MODEL_DIMENSION,\n",
        "                              inner_layer_dimension=MODEL_INNER_LAYER_DIMENSION,\n",
        "                              number_of_layers=MODEL_NUMBER_OF_LAYERS,\n",
        "                              number_of_heads=MODEL_NUMBER_OF_HEADS,\n",
        "                              src_vocabulary_size=src_vocab_size,\n",
        "                              trg_vocabulary_size=trg_vocab_size,\n",
        "                              dropout_probability=MODEL_DROPOUT_PROBABILITY\n",
        "                            )\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def greedy_decode(model, source, source_mask, src_tokenizer, trg_tokenizer, max_len, device):\n",
        "    sos_idx = trg_tokenizer.token_to_id('[SOS]')\n",
        "    eos_idx = trg_tokenizer.token_to_id('[EOS]')\n",
        "\n",
        "    # Precompute the encoder output and reuse it for every step\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "    # Initialize the decoder input with the sos token\n",
        "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
        "\n",
        "    while True:\n",
        "        if decoder_input.size(1) == max_len:\n",
        "            break\n",
        "\n",
        "        # build mask for target\n",
        "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
        "\n",
        "        # calculate output\n",
        "        out = model.decode(encoder_output, decoder_input, source_mask, decoder_mask)\n",
        "\n",
        "        # get next token\n",
        "        prob = model.linear_projection(out[:, -1])\n",
        "        prob = model.softmax(prob)\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        decoder_input = torch.cat(\n",
        "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
        "        )\n",
        "\n",
        "        if next_word == eos_idx:\n",
        "            break\n",
        "\n",
        "    return decoder_input.squeeze(0)\n",
        "\n",
        "\n",
        "def run_validation(model, validation_ds, src_tokenizer, trg_tokenizer, max_len, device, print_msg, global_step, writer, num_examples=2):\n",
        "    model.eval()\n",
        "    count = 0\n",
        "\n",
        "    source_texts = []\n",
        "    expected = []\n",
        "    predicted = []\n",
        "\n",
        "    try:\n",
        "        # get the console window width\n",
        "        with os.popen('stty size', 'r') as console:\n",
        "            _, console_width = console.read().split()\n",
        "            console_width = int(console_width)\n",
        "    except:\n",
        "        # If we can't get the console width, use 80 as default\n",
        "        console_width = 80\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_ds:\n",
        "            count += 1\n",
        "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
        "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
        "\n",
        "            # check that the batch size is 1\n",
        "            assert encoder_input.size(\n",
        "                0) == 1, \"Batch size must be 1 for validation\"\n",
        "\n",
        "            model_out = greedy_decode(model, encoder_input, encoder_mask, src_tokenizer, trg_tokenizer, max_len, device)\n",
        "\n",
        "            source_text = batch[\"src_text\"][0]\n",
        "            target_text = batch[\"trg_text\"][0]\n",
        "            model_out_text = trg_tokenizer.decode(model_out.detach().cpu().numpy())\n",
        "\n",
        "            source_texts.append(source_text)\n",
        "            expected.append(target_text)\n",
        "            predicted.append(model_out_text)\n",
        "\n",
        "            # Print the source, target and model output\n",
        "            print_msg('-'*console_width)\n",
        "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
        "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
        "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
        "\n",
        "            if count == num_examples:\n",
        "                print_msg('-'*console_width)\n",
        "                break\n",
        "\n",
        "    if writer:\n",
        "        # Evaluate the character error rate\n",
        "        # Compute the char error rate\n",
        "        metric = torchmetrics.CharErrorRate()\n",
        "        cer = metric(predicted, expected)\n",
        "        writer.add_scalar('validation cer', cer, global_step)\n",
        "        writer.flush()\n",
        "\n",
        "        # Compute the word error rate\n",
        "        metric = torchmetrics.WordErrorRate()\n",
        "        wer = metric(predicted, expected)\n",
        "        writer.add_scalar('validation wer', wer, global_step)\n",
        "        writer.flush()\n",
        "\n",
        "        # Compute the BLEU metric\n",
        "        metric = torchmetrics.BLEUScore()\n",
        "        bleu = metric(predicted, expected)\n",
        "        writer.add_scalar('validation BLEU', bleu, global_step)\n",
        "        writer.flush()\n",
        "\n",
        "        print(f\"CER: {cer:.4f} | WER: {wer:.4f} | BLEU: {bleu:.4f}\")\n",
        "\n",
        "\n",
        "def learning_rate(step):\n",
        "    if step == 0:\n",
        "        step = 1\n",
        "    return MODEL_DIMENSION ** (-0.5) * min(step ** (-0.5), step * WARMUP_STEPS ** (-1.5))\n",
        "\n",
        "\n",
        "def train_model():\n",
        "    # Define the device\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Using device:\", device)\n",
        "    if (device == 'cuda'):\n",
        "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
        "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
        "    device = torch.device(device)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Make sure the weights folder exists\n",
        "    Path(f\"{DATASET_NAME}_{MODEL_FOLDER}\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_dataloader, val_dataloader, src_tokenizer, trg_tokenizer = get_dataset()\n",
        "    model = get_transformer(src_tokenizer.get_vocab_size(), trg_tokenizer.get_vocab_size()).to(device)\n",
        "\n",
        "    # Tensorboard\n",
        "    writer = SummaryWriter(EXPERIMENT_FOLDER)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(BETA1, BETA2), eps=EPSILON)\n",
        "    #lr_scheduler = LambdaLR(optimizer=optimizer, lr_lambda=lambda step: learning_rate(step))\n",
        "\n",
        "    # If the user specified a model to preload before training, load it\n",
        "    initial_epoch = 1\n",
        "    global_step = 0\n",
        "    preload = MODEL_PRELOAD\n",
        "    model_filename = latest_weights_file_path() if preload == 'latest' else get_weights_file_path(preload) if preload else None\n",
        "    if model_filename:\n",
        "        print(f'Preloading model {model_filename}')\n",
        "        state = torch.load(model_filename)\n",
        "        model.load_state_dict(state['model_state_dict'])\n",
        "        initial_epoch = state['epoch'] + 1\n",
        "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        global_step = state['global_step']\n",
        "    else:\n",
        "        print('No model to preload, starting from scratch')\n",
        "\n",
        "    loss_function = nn.CrossEntropyLoss(ignore_index=trg_tokenizer.token_to_id('[PAD]'), label_smoothing=MODEL_LABEL_SMOOTHING_VALUE).to(device)\n",
        "\n",
        "    for epoch in range(initial_epoch, NUMBER_OF_EPOCHS + 1):\n",
        "        torch.cuda.empty_cache()\n",
        "        model.train()\n",
        "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
        "        for batch in batch_iterator:\n",
        "\n",
        "            encoder_input = batch['encoder_input'].to(device) # (B, seq_len)\n",
        "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
        "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
        "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
        "\n",
        "            # Run the tensors through the encoder, decoder and the projection layer\n",
        "            model_output = model(encoder_input, decoder_input, encoder_mask, decoder_mask)\n",
        "\n",
        "            # Compare the output with the label\n",
        "            label = batch['label'].to(device) # (B, seq_len)\n",
        "\n",
        "            # Compute the loss using a simple cross entropy\n",
        "            loss = loss_function(model_output.view(-1, trg_tokenizer.get_vocab_size()), label.view(-1))\n",
        "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "            # Log the loss\n",
        "            writer.add_scalar('train loss', loss.item(), global_step)\n",
        "            writer.flush()\n",
        "\n",
        "            # Backpropagate the loss\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            # Update the weights\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            #lr_scheduler.step()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        # Run validation at the end of every epoch\n",
        "        run_validation(model, val_dataloader, src_tokenizer, trg_tokenizer, SEQUENCE_LENGTH, device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
        "\n",
        "        # Save the model at the end of every epoch\n",
        "        model_filename = get_weights_file_path(f\"{epoch:02d}\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'global_step': global_step\n",
        "        }, model_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "69a0c33f01674dbc8fa3c0a9bafa0de4",
            "4777fb29cc9d4b348e71f8840d4e2c5d",
            "28f9c6ddb51b4c9cb282830850446116",
            "a8782bb9e1df497aa14da8343db55c17",
            "2f1a0981877b47c697c2a9782e31f710",
            "b6cd20a8e6904498bfaea93add9deeb4",
            "0a904386cfd6486c8c0c87fa869dff4d",
            "1db1528faf2342cb9fd90d4fbd5606eb",
            "042cfbcb1929455a92d9e3cb3bcba2a2",
            "6c8b5e518503489a88448bdaa3a7e226",
            "8325537a12c140fdbaef46001e36c90c"
          ]
        },
        "id": "M0jId-UHK86g",
        "outputId": "d45e9bb8-c99b-424f-df3c-3deb87789afd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Device name: Tesla T4\n",
            "Device memory: 14.74127197265625 GB\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69a0c33f01674dbc8fa3c0a9bafa0de4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/31771 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset filtred: 31771 examples left\n",
            "Max length of source sentence: 309\n",
            "Max length of target sentence: 320\n",
            "No model to preload, starting from scratch\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 01: 100%|██████████| 3177/3177 [07:42<00:00,  6.86it/s, loss=5.609]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"My Lord, excuse me! I speak as I can; I restrain myself.\n",
            "    TARGET: -- Milord, excusez-moi! je parle comme je puis; je me contiens.\n",
            " PREDICTED: -- Je me me me me me me me , je me , je me , je me .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Afterwards he declared that the mates could not even know whether it was round or square, he had rushed along so swiftly.\n",
            "    TARGET: Ensuite, il donna sa parole que les camarades ne devaient pas meme savoir si elle l'avait rond ou carré, tellement il galopait raide.\n",
            " PREDICTED: Il était bien que si bien qu ' il ne pouvait être pas que si bien qu ' il ne pouvait être si bien qu ' il ne pouvait être .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 02: 100%|██████████| 3177/3177 [07:46<00:00,  6.81it/s, loss=4.983]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"Yes, sir.\"\n",
            "    TARGET: – Oui, monsieur.\n",
            " PREDICTED: -- Oui , monsieur .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"Always 'duty.' I am sick of the word.\n",
            "    TARGET: Toujours les devoirs, je suis assommé de ces mots-là.\n",
            " PREDICTED: -- Je suis bien que je suis bien que je suis le plus de la plus .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 03: 100%|██████████| 3177/3177 [07:46<00:00,  6.82it/s, loss=5.276]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: 'What pleasure, what instinct leads them to betray us?\n",
            "    TARGET: Quel plaisir, quel instinct les portent à nous tromper !\n",
            " PREDICTED: – Que - vous , que nous avons dit - il , de nous ?\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: – Moi, Madame !\n",
            "    TARGET: \"I, Signora!\n",
            " PREDICTED: \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \"\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 04: 100%|██████████| 3177/3177 [07:45<00:00,  6.82it/s, loss=5.404]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Bovary during this time did not dare to stir from his house.\n",
            "    TARGET: Bovary, pendant ce temps-là, n’osait bouger de sa maison.\n",
            " PREDICTED: Il ne fallait pas de temps à ce temps , de temps , de la maison .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"The frigate?\" Conseil replied, rolling over on his back. \"I think master had best not depend on it to any great extent!\"\n",
            "    TARGET: -- La frégate ! répondit Conseil en se retournant sur le dos, je crois que monsieur fera bien de ne pas trop compter sur elle !\n",
            " PREDICTED: -- Le maître ! répondit Conseil , Conseil , sans doute , qui n ' avait pas de la mer ?\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 05: 100%|██████████| 3177/3177 [07:45<00:00,  6.83it/s, loss=5.383]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: They no longer felt the cold, these burning words had warmed them to the bone.\n",
            "    TARGET: Ils ne sentaient plus le froid, ces ardentes paroles les avaient chauffés aux entrailles.\n",
            " PREDICTED: Ils avaient des mots , les paroles , les , les .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: I approached my cheek to her lips: she would not touch it. She said I oppressed her by leaning over the bed, and again demanded water. As I laid her down--for I raised her and supported her on my arm while she drank--I covered her ice-cold and clammy hand with mine: the feeble fingers shrank from my touch--the glazing eyes shunned my gaze.\n",
            "    TARGET: J'approchai ma joue de ses lèvres, mais elle ne la toucha pas: elle me dit que je l'oppressais en me penchant sur son lit, et me redemanda de l'eau; lorsque je la recouchai, car je l'avais soulevée avec mon bras pendant qu'elle buvait, je pris dans mes mains ses mains froides; mais ses faibles doigts essayèrent de m'échapper, ses yeux vitreux évitèrent les miens.\n",
            " PREDICTED: Elle me dit que je ma main sur la main sur la main , et je me dis que je m ' ; elle me fit tomber sur la main sur la main , je me dit qu ' elle ne me fit pas de la main sur le bras ; je m ' y , je m ' , je m ' , je m ' sur le bras , et je m ' , je m ' , je m ' , je m ' , et je m ' , je m ' , je m ' sur la main sur la main sur la main sur la main sur la main sur la main sur la main sur le bras .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 06: 100%|██████████| 3177/3177 [07:45<00:00,  6.83it/s, loss=4.864]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Mais aussitôt que Clélia n’eut plus d’inquiétudes de ce côté, elle fut plus cruellement agitée encore par ses justes remords.\n",
            "    TARGET: But as soon as Clelia had no longer any anxiety in that direction, she was even more cruelly tormented by her just remorse.\n",
            " PREDICTED: the of the of the of the of the of the of the of the of the .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Afterwards he declared that the mates could not even know whether it was round or square, he had rushed along so swiftly.\n",
            "    TARGET: Ensuite, il donna sa parole que les camarades ne devaient pas meme savoir si elle l'avait rond ou carré, tellement il galopait raide.\n",
            " PREDICTED: Il était si il ne pouvait pas même s ’ il était si bien , il ne pouvait pas même , il ne pouvait pas même s ’ il était si le voir si bien , il était si le pouvait même , il ne pouvait même , il était si le voir .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 07: 100%|██████████| 3177/3177 [07:45<00:00,  6.82it/s, loss=4.533]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"What’s the matter now, Planchet?\" demanded d’Artagnan.\n",
            "    TARGET: -- Qu'y a-t-il donc? demanda d'Artagnan.\n",
            " PREDICTED: -- Qu ' est - ce que vous ? dit d ' Artagnan .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: I could not help it. I thought of him now--in his room--watching the sunrise; hoping I should soon come to say I would stay with him and be his.\n",
            "    TARGET: Je ne pouvais m'empêcher de songer avec agonie à ce que j'avais laissé, à celui qui épiait dans sa chambre le lever du soleil, espérant me voir bientôt arriver pour lui dire que je voulais bien lui appartenir et rester près de lui.\n",
            " PREDICTED: Je ne pouvais pas me dire ; je lui dis que je ne pouvais pas me dire ; je lui dire , et je lui dis que je ne pouvais pas me dire .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 08: 100%|██████████| 3177/3177 [07:46<00:00,  6.81it/s, loss=4.337]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Il était immédiatement suivi d’un valet de chambre apportant une tasse de café infiniment petite, soutenue par un pied d’argent en filigrane ; et toutes les demi-heures un maître d’hôtel, portant épée et habit magnifique à la française, venait offrir des glaces.\n",
            "    TARGET: He was immediately followed by a footman carrying an infinitesimal cup of coffee, supported on a stem of silver filigree; and every half hour a butler, wearing a sword and a magnificent coat, in the French style, brought round ices.\n",
            " PREDICTED: He was a of a , and , and a of the of a of a of a of a .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The commendation bestowed on him by Mrs. Reynolds was of no trifling nature.\n",
            "    TARGET: Reynolds n’étaient pas de qualité ordinaire et quelle louange a plus de valeur que celle d’un serviteur intelligent ?\n",
            " PREDICTED: Le jour de Mme Fairfax fut par une nature de lui .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 09: 100%|██████████| 3177/3177 [07:46<00:00,  6.81it/s, loss=4.572]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"That is my name,\" said Athos, quietly.\n",
            "    TARGET: -- C'est mon nom, dit tranquillement Athos.\n",
            " PREDICTED: -- C ' est mon nom , dit Athos .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: I was thus steeped in the marvellous ecstasy which all high summits develop in the mind; and now without giddiness, for I was beginning to be accustomed to these sublime aspects of nature. My dazzled eyes were bathed in the bright flood of the solar rays.\n",
            "    TARGET: Je me plongeais ainsi dans cette prestigieuse extase que donnent les hautes cimes, et cette fois, sans vertige, car je m'accoutumais enfin à ces sublimes contemplations.\n",
            " PREDICTED: Mon esprit était ainsi , sans être ainsi ; car je me ainsi , sans être ainsi , je me ainsi , et je me mis à la nature , sans être ainsi que les yeux de la nature de l ' esprit de l ' état de la nature .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 10: 100%|██████████| 3177/3177 [07:45<00:00,  6.82it/s, loss=4.246]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: I spent the evening in reading, writing, and thinking.\n",
            "    TARGET: Je passai la soirée à lire, à écrire, à penser.\n",
            " PREDICTED: Je me mis en effet , je me mis dans la soirée .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: I thought I might have retorted the question on him who put it: but I would not take that freedom.\n",
            "    TARGET: Je pensai que j'aurais bien pu lui retourner sa question; mais n'osant pas prendre cette liberté, je lui répondis:\n",
            " PREDICTED: Je ne savais pas que je ne savais pas que je ne savais pas que je ne savais pas que je ne savais pas que je ne savais pas que je ne savais pas que je ne savais pas que je ne savais pas que je ne savais pas que je ne savais pas que je ne savais pas que je ne savais pas que je ne savais pas que je ne savais pas que je ne savais pas que je ne savais pas que je ne savais pas que je ne savais pas .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 11: 100%|██████████| 3177/3177 [07:46<00:00,  6.82it/s, loss=4.689]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: But in any case it was a blow to him, and it would take him some time before he could prepare himself to meet it.\n",
            "    TARGET: Mais en toute éventualité, ce serait un coup, et un peu de temps lui serait nécessaire pour qu’il pût s’y préparer.\n",
            " PREDICTED: Mais il ne pouvait le faire , il lui un coup d ' oeil , et il l ' avait pu le faire un coup .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Miss Cunegund could scarce refrain from laughing at the good old woman, and thought it droll enough to pretend to a greater share of misfortunes than her own.\n",
            "    TARGET: Cunégonde se mit presque à rire, et trouva cette bonne femme fort plaisante de prétendre être plus malheureuse qu'elle.\n",
            " PREDICTED: Mlle Temple , si la vieille femme pouvait la rendre assez grande pour la vieille , et , en faire une vieille femme de sa vieille femme .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 12: 100%|██████████| 3177/3177 [07:46<00:00,  6.81it/s, loss=5.230]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Then added, repulsing him with a languid movement—\n",
            "    TARGET: Puis, elle ajoutait en le repoussant d’un geste langoureux:\n",
            " PREDICTED: Alors , le , avec une .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: They've all cleared out of the place.\"\n",
            "    TARGET: Tous ont fichu le camp.\n",
            " PREDICTED: Ils ont fait tout le lieu de la place .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 13: 100%|██████████| 3177/3177 [07:46<00:00,  6.82it/s, loss=4.889]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The result of the rally in the last round had convinced his seconds that when it came to give-and-take hitting, their hardy and powerful man was likely to have the better of it.\n",
            "    TARGET: Le résultat du repos après le dernier round avait convaincu les seconds que leur champion, avec son endurance et sa vigueur, devait avoir le dessus quand il s'agissait de recevoir et de rendre des coups.\n",
            " PREDICTED: Le dernier homme de ses et les de leur côté , quand il fut mieux pour prendre le dernier homme , il était évident que les de leur homme .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: We moved cautiously along the track as if we were bound for the house, but Holmes halted us when we were about two hundred yards from it.\n",
            "    TARGET: Nous avançâmes avec précaution sur le chemin comme si nous nous rendions à la maison, mais Holmes stoppa à deux cents mètres d’elle.\n",
            " PREDICTED: Nous de deux cents cents cents cents milles , mais comme nous à la maison , quand nous à la maison , nous de deux cents pas .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 14: 100%|██████████| 3177/3177 [07:44<00:00,  6.84it/s, loss=4.334]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"Ay, pray do,\" said Candide, \"and be sure you make them sensible of the horrid barbarity of boiling and roasting human creatures, and how little of Christianity there is in such practices.\"\n",
            "    TARGET: Ne manquez pas, dit Candide, de leur représenter quelle est l'inhumanité affreuse de faire cuire des hommes, et combien cela est peu chrétien.\n",
            " PREDICTED: Candide est sûr de les , dit Candide , et il y a de les , et les de la petite personne de les .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: In single file they still went on without a word, by the tiny flame of the lamps.\n",
            "    TARGET: Un par un, ils allaient, ils allaient toujours, sans une parole, avec les petites flammes des lampes.\n",
            " PREDICTED: En effet , les sept heures , sans rien dire un seul mot , ils par le mot de la parole .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 15: 100%|██████████| 3177/3177 [07:43<00:00,  6.85it/s, loss=5.144]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The sketch of Rosamond's portrait pleased him highly: he said I must make a finished picture of it. He insisted, too, on my coming the next day to spend the evening at Vale Hall.\n",
            "    TARGET: L'esquisse du portrait de Rosamonde lui plut beaucoup; il me demanda d'en faire une peinture aussi perfectionnée que possible; il me pria aussi de venir le lendemain passer la soirée à Vale-Hall.\n",
            " PREDICTED: Il me dit que le lendemain , il devait être trop vite , et il fallait en lui faire une parole , en lui une parole .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"Because we have very important matters to communicate to one another, and it was impossible to talk five minutes in that inn without being annoyed by all those importunate fellows, who keep coming in, saluting you, and addressing you. Here at least,\" said Athos, pointing to the bastion, \"they will not come and disturb us.\"\n",
            "    TARGET: Parce que nous avons des choses fort importantes à nous dire, et qu'il était impossible de causer cinq minutes dans cette auberge avec tous ces importuns qui vont, qui viennent, qui saluent, qui accostent; ici, du moins, continua Athos en montrant le bastion, on ne viendra pas nous déranger.\n",
            " PREDICTED: -- Parce que nous à la fois , reprit Athos en se tournant vers un autre , et que vous avez dit Athos , et que vous avez été par un autre , et que vous avez sans qu ' il y avait eu un autre , et sans qu ' ils ne nous pas , et sans avoir eu cinq minutes à l ' auberge .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "2c3ca097d0334ed8b654fc4991ad8281",
            "ffc00025ac85453492316e4583a35b67",
            "ecb947c47ed74510ab6a1ad080ea7f92",
            "364f21ea36de46fe98ab7e00205e586c",
            "4f149c2612ac462cadead79a648a39fe",
            "1b7309cec8fe4dde8af5f78b58b917a8",
            "17f22037ce4f4a02a91facd9858c7df7",
            "73a7d581017b44c9b74cbccd76c8614c",
            "1edead9841a04ac3bd504101bb894c98",
            "d4591cd734e2449e887b2be1e251be9c",
            "1f3e1e98921046869de4520b1e1df3b2"
          ]
        },
        "id": "QXgnS1OFN4Po",
        "outputId": "6fa43be7-ceb1-4d05-bfc3-bcf561fd2c89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c3ca097d0334ed8b654fc4991ad8281",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/31771 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset filtred: 31771 examples left\n",
            "Max length of source sentence: 376\n",
            "Max length of target sentence: 384\n",
            "Testing overfitting on single batch...\n",
            "Step 0: Loss = 10.2242\n",
            "Step 20: Loss = 1.0047\n",
            "Step 40: Loss = 0.1244\n",
            "Step 60: Loss = 0.0407\n",
            "Step 80: Loss = 0.0330\n",
            "Model CAN overfit - the architecture is working\n"
          ]
        }
      ],
      "source": [
        "def test_overfitting():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Using device:\", device)\n",
        "    train_dataloader, val_dataloader, src_tokenizer, trg_tokenizer = get_dataset()\n",
        "    model = get_transformer(src_tokenizer.get_vocab_size(), trg_tokenizer.get_vocab_size()).to(device)\n",
        "    # Prenez 1 seul batch\n",
        "    single_batch = next(iter(train_dataloader))\n",
        "\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    print(\"Testing overfitting on single batch...\")\n",
        "    for step in range(100):\n",
        "        encoder_input = single_batch['encoder_input'].to(device)\n",
        "        decoder_input = single_batch['decoder_input'].to(device)\n",
        "        encoder_mask = single_batch['encoder_mask'].to(device)\n",
        "        decoder_mask = single_batch['decoder_mask'].to(device)\n",
        "        label = single_batch['label'].to(device)\n",
        "\n",
        "        model_output = model(encoder_input, decoder_input, encoder_mask, decoder_mask)\n",
        "        loss = nn.CrossEntropyLoss(ignore_index=trg_tokenizer.token_to_id('[PAD]'))(\n",
        "            model_output.view(-1, trg_tokenizer.get_vocab_size()),\n",
        "            label.view(-1)\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if step % 20 == 0:\n",
        "            print(f\"Step {step}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "    if loss.item() < 0.1:\n",
        "        print(\"Model CAN overfit - the architecture is working\")\n",
        "    else:\n",
        "        print(\"Model CANNOT overfit - there's a fundamental problem\")\n",
        "\n",
        "test_overfitting()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNE7Nev2g5UZ",
        "outputId": "96cf6cb3-e5bc-4d09-b971-205a62884943"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Copied folder Helsinki-NLP to Google Drive\n",
            "Copied folder data to Google Drive\n",
            "Copied folder runs to Google Drive\n",
            "Copied folder models to Google Drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Copy entire folder to Google Drive\n",
        "source_folder = 'Helsinki-NLP'\n",
        "destination_folder = '/content/drive/MyDrive/transformer_project/Helsinki-NLP'\n",
        "\n",
        "if os.path.exists(source_folder):\n",
        "    shutil.copytree(source_folder, destination_folder, dirs_exist_ok=True)\n",
        "    print(f\"Copied folder {source_folder} to Google Drive\")\n",
        "    shutil.copytree('data', destination_folder, dirs_exist_ok=True)\n",
        "    print(f\"Copied folder data to Google Drive\")\n",
        "    shutil.copytree('runs', destination_folder, dirs_exist_ok=True)\n",
        "    print(f\"Copied folder runs to Google Drive\")\n",
        "    shutil.copytree('models', destination_folder, dirs_exist_ok=True)\n",
        "    print(f\"Copied folder models to Google Drive\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31090,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "042cfbcb1929455a92d9e3cb3bcba2a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a904386cfd6486c8c0c87fa869dff4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17f22037ce4f4a02a91facd9858c7df7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b7309cec8fe4dde8af5f78b58b917a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1db1528faf2342cb9fd90d4fbd5606eb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1edead9841a04ac3bd504101bb894c98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f3e1e98921046869de4520b1e1df3b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28f9c6ddb51b4c9cb282830850446116": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1db1528faf2342cb9fd90d4fbd5606eb",
            "max": 31771,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_042cfbcb1929455a92d9e3cb3bcba2a2",
            "value": 31771
          }
        },
        "2c3ca097d0334ed8b654fc4991ad8281": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ffc00025ac85453492316e4583a35b67",
              "IPY_MODEL_ecb947c47ed74510ab6a1ad080ea7f92",
              "IPY_MODEL_364f21ea36de46fe98ab7e00205e586c"
            ],
            "layout": "IPY_MODEL_4f149c2612ac462cadead79a648a39fe"
          }
        },
        "2f1a0981877b47c697c2a9782e31f710": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "364f21ea36de46fe98ab7e00205e586c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4591cd734e2449e887b2be1e251be9c",
            "placeholder": "​",
            "style": "IPY_MODEL_1f3e1e98921046869de4520b1e1df3b2",
            "value": " 31771/31771 [00:02&lt;00:00, 10724.56 examples/s]"
          }
        },
        "4777fb29cc9d4b348e71f8840d4e2c5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6cd20a8e6904498bfaea93add9deeb4",
            "placeholder": "​",
            "style": "IPY_MODEL_0a904386cfd6486c8c0c87fa869dff4d",
            "value": "Filter: 100%"
          }
        },
        "4f149c2612ac462cadead79a648a39fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69a0c33f01674dbc8fa3c0a9bafa0de4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4777fb29cc9d4b348e71f8840d4e2c5d",
              "IPY_MODEL_28f9c6ddb51b4c9cb282830850446116",
              "IPY_MODEL_a8782bb9e1df497aa14da8343db55c17"
            ],
            "layout": "IPY_MODEL_2f1a0981877b47c697c2a9782e31f710"
          }
        },
        "6c8b5e518503489a88448bdaa3a7e226": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73a7d581017b44c9b74cbccd76c8614c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8325537a12c140fdbaef46001e36c90c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8782bb9e1df497aa14da8343db55c17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c8b5e518503489a88448bdaa3a7e226",
            "placeholder": "​",
            "style": "IPY_MODEL_8325537a12c140fdbaef46001e36c90c",
            "value": " 31771/31771 [00:02&lt;00:00, 11516.23 examples/s]"
          }
        },
        "b6cd20a8e6904498bfaea93add9deeb4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4591cd734e2449e887b2be1e251be9c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecb947c47ed74510ab6a1ad080ea7f92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73a7d581017b44c9b74cbccd76c8614c",
            "max": 31771,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1edead9841a04ac3bd504101bb894c98",
            "value": 31771
          }
        },
        "ffc00025ac85453492316e4583a35b67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b7309cec8fe4dde8af5f78b58b917a8",
            "placeholder": "​",
            "style": "IPY_MODEL_17f22037ce4f4a02a91facd9858c7df7",
            "value": "Filter: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
