{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31090,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cf0fc630575d4182a51cc86f6a384d5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28d29988957d4a7db4805dd5964cc289",
              "IPY_MODEL_7c56421dd1174ae1b2e93b880bd4fcbd",
              "IPY_MODEL_576fa42f771a42749a77c4c1627329be"
            ],
            "layout": "IPY_MODEL_d6b61b8fc7d2410288610e76a59e2390"
          }
        },
        "28d29988957d4a7db4805dd5964cc289": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0be4f546fe448438a1b9e0621c675d0",
            "placeholder": "​",
            "style": "IPY_MODEL_e413d765a2484e1f889608356a39098f",
            "value": "Filter: 100%"
          }
        },
        "7c56421dd1174ae1b2e93b880bd4fcbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5ec49313a3647a2af2905441160ecf8",
            "max": 31771,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d9d5b11e36249bcad85d5413a44b8f2",
            "value": 31771
          }
        },
        "576fa42f771a42749a77c4c1627329be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6f0e4e720064572acc3966a3b468aa0",
            "placeholder": "​",
            "style": "IPY_MODEL_8b17b678082642e9937a62a5f0ab5330",
            "value": " 31771/31771 [00:03&lt;00:00, 11450.37 examples/s]"
          }
        },
        "d6b61b8fc7d2410288610e76a59e2390": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0be4f546fe448438a1b9e0621c675d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e413d765a2484e1f889608356a39098f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5ec49313a3647a2af2905441160ecf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d9d5b11e36249bcad85d5413a44b8f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d6f0e4e720064572acc3966a3b468aa0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b17b678082642e9937a62a5f0ab5330": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb73937aea114be1a435dc4ebc91e8d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc7eb54f006145e3b810132377760281",
              "IPY_MODEL_d5781cf27aa341adb2144f27d0d7c58a",
              "IPY_MODEL_30510dc7c0c64d709be206db6f23ec67"
            ],
            "layout": "IPY_MODEL_a7abd66af4274eb69148f12b696ea674"
          }
        },
        "bc7eb54f006145e3b810132377760281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcd96194a4c04dfd8964ca67de5141eb",
            "placeholder": "​",
            "style": "IPY_MODEL_fd61d8fa3cb8494ca2944d22202c995d",
            "value": "Filter: 100%"
          }
        },
        "d5781cf27aa341adb2144f27d0d7c58a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2774dceb9d2c4049b0dc6234f1dee215",
            "max": 31771,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_573e999c4d41471185cd860c688f6f96",
            "value": 31771
          }
        },
        "30510dc7c0c64d709be206db6f23ec67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afb70919916a49a88ce3f8e107dbbdf2",
            "placeholder": "​",
            "style": "IPY_MODEL_f626be653d284ff9afd85587f34d74b9",
            "value": " 31771/31771 [00:05&lt;00:00, 10967.48 examples/s]"
          }
        },
        "a7abd66af4274eb69148f12b696ea674": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcd96194a4c04dfd8964ca67de5141eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd61d8fa3cb8494ca2944d22202c995d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2774dceb9d2c4049b0dc6234f1dee215": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "573e999c4d41471185cd860c688f6f96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "afb70919916a49a88ce3f8e107dbbdf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f626be653d284ff9afd85587f34d74b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sacremoses torchmetrics"
      ],
      "metadata": {
        "id": "JYy22wYG5Cpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1c1b0b6-4ead-415d-c6e8-c3d6ae9f4dfa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.1-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2024.11.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sacremoses) (4.67.1)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.8.1-py3-none-any.whl (982 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.0/983.0 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.15.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 sacremoses-0.1.1 torchmetrics-1.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# Huggingface datasets and tokenizers\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torchmetrics\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T13:30:19.784975Z",
          "iopub.execute_input": "2025-08-08T13:30:19.785643Z",
          "iopub.status.idle": "2025-08-08T13:30:19.789877Z",
          "shell.execute_reply.started": "2025-08-08T13:30:19.785618Z",
          "shell.execute_reply": "2025-08-08T13:30:19.789127Z"
        },
        "id": "nwKFh_PV5A5j"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Transformer model parameters\n",
        "MODEL_NUMBER_OF_LAYERS = 3              # paper value : 6\n",
        "MODEL_DIMENSION = 256                   # 512\n",
        "MODEL_NUMBER_OF_HEADS = 4               # 8\n",
        "MODEL_INNER_LAYER_DIMENSION = 1024      # 2048\n",
        "MODEL_DROPOUT_PROBABILITY = 0.1         # 0.1\n",
        "MODEL_LABEL_SMOOTHING_VALUE = 0.1       # 0.1\n",
        "SEQUENCE_LENGTH = 500\n",
        "\n",
        "# Training parameters\n",
        "BATCH_SIZE = 8\n",
        "NUMBER_OF_EPOCHS = 10\n",
        "BETA1 = 0.9                             # 0.9\n",
        "BETA2 = 0.98                            # 0.98\n",
        "EPSILON = 1e-9                          # 1e-9\n",
        "WARMUP_STEPS = 4000                     # 4000\n",
        "\n",
        "# Dataset parameters\n",
        "DATASET_NAME = \"Helsinki-NLP/opus_books\"\n",
        "SOURCE_LANGUAGE = \"en\"\n",
        "TARGET_LANGUAGE = \"fr\"\n",
        "\n",
        "# Saving parameters\n",
        "MODEL_FOLDER = \"weights\"\n",
        "MODEL_BASENAME = \"tmodel_\"\n",
        "MODEL_PRELOAD = \"latest\"\n",
        "EXPERIMENT_FOLDER = \"runs/tmodel\"\n",
        "\n",
        "# Special tokens\n",
        "UNK_TOKEN = '[UNK]'\n",
        "SOS_TOKEN = '[SOS]'\n",
        "EOS_TOKEN = '[EOS]'\n",
        "PAD_TOKEN = \"[PAD]\"\n",
        "\n",
        "CHECKPOINTS_PATH = os.path.join(os.getcwd(), 'models', 'checkpoints') # semi-trained models during training will be dumped here\n",
        "BINARIES_PATH = os.path.join(os.getcwd(), 'models', 'binaries') # location where trained models are located\n",
        "DATA_DIR_PATH = os.path.join(os.getcwd(), 'data') # training data will be stored here\n",
        "\n",
        "os.makedirs(CHECKPOINTS_PATH, exist_ok=True)\n",
        "os.makedirs(BINARIES_PATH, exist_ok=True)\n",
        "os.makedirs(DATA_DIR_PATH, exist_ok=True)\n",
        "\n",
        "def get_weights_file_path(epoch: str):\n",
        "    model_folder = f\"{DATASET_NAME}_{MODEL_FOLDER}\"\n",
        "    model_filename = f\"{MODEL_BASENAME}{epoch}.pt\"\n",
        "    return str(Path('.') / model_folder / model_filename)\n",
        "\n",
        "# Find the latest weights file in the weights folder\n",
        "def latest_weights_file_path():\n",
        "    model_folder = f\"{DATASET_NAME}_{MODEL_FOLDER}\"\n",
        "    model_filename = f\"{MODEL_BASENAME}*\"\n",
        "    weights_files = list(Path(model_folder).glob(model_filename))\n",
        "    if len(weights_files) == 0:\n",
        "        return None\n",
        "    weights_files.sort()\n",
        "    return str(weights_files[-1])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T13:30:19.902956Z",
          "iopub.execute_input": "2025-08-08T13:30:19.903171Z",
          "iopub.status.idle": "2025-08-08T13:30:19.920365Z",
          "shell.execute_reply.started": "2025-08-08T13:30:19.903154Z",
          "shell.execute_reply": "2025-08-08T13:30:19.919775Z"
        },
        "id": "nGAC60AY5A5p"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocabulary_size, model_dimension):\n",
        "        super().__init__()\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        self.model_dimension = model_dimension\n",
        "        self.embedding = nn.Embedding(vocabulary_size, model_dimension)\n",
        "\n",
        "    def forward(self, src_token_ids):\n",
        "        return self.embedding(src_token_ids) * math.sqrt(self.model_dimension)\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, model_dimension, dropout_probability, max_sequence_length=5000):\n",
        "        super().__init__()\n",
        "        self.sequence_length = max_sequence_length\n",
        "        self.model_dimension = model_dimension\n",
        "        self.dropout = nn.Dropout(dropout_probability)\n",
        "\n",
        "        positional_encoding = torch.zeros(max_sequence_length, model_dimension)\n",
        "\n",
        "        positions = torch.arange(0, max_sequence_length, 1, dtype=float)\n",
        "        positions = torch.unsqueeze(positions, 1)\n",
        "        denominator = torch.exp(torch.arange(0, model_dimension, 2, dtype=float) * -math.log(10000.) / model_dimension)\n",
        "\n",
        "        positional_encoding[:,0::2] = torch.sin(positions * denominator)\n",
        "        positional_encoding[:,1::2] = torch.cos(positions * denominator)\n",
        "\n",
        "        positional_encoding = positional_encoding.unsqueeze(0)\n",
        "        self.register_buffer(\"positional_encoding\", positional_encoding)\n",
        "\n",
        "    def forward(self, src_embedded):\n",
        "        return self.dropout(src_embedded + self.positional_encoding[:, :src_embedded.shape[1]])\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, key_dimension, value_dimension):\n",
        "        super().__init__()\n",
        "        self.key_dimension = key_dimension\n",
        "        self.value_dimension = value_dimension\n",
        "\n",
        "    def forward(self, queries, keys, values, mask=None):\n",
        "        dot_product = torch.matmul(queries, keys.transpose(-2, -1))\n",
        "        dot_product /= math.sqrt(self.key_dimension)\n",
        "\n",
        "        if mask is not None:\n",
        "            dot_product.masked_fill_(mask == 0, float(\"-inf\"))\n",
        "\n",
        "        weights = dot_product.softmax(dim=-1)\n",
        "\n",
        "        return torch.matmul(weights, values), weights\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, model_dimension, number_of_heads, save_weigths=False):\n",
        "        super().__init__()\n",
        "        self.model_dimension = model_dimension\n",
        "        self.number_of_heads = number_of_heads\n",
        "        self.key_dimension = model_dimension // number_of_heads\n",
        "        self.value_dimension = model_dimension // number_of_heads\n",
        "\n",
        "        self.linear_queries = nn.Linear(model_dimension, model_dimension) # W_Q\n",
        "        self.linear_keys = nn.Linear(model_dimension, model_dimension) # W_K\n",
        "        self.linear_values = nn.Linear(model_dimension, model_dimension) # W_V\n",
        "\n",
        "        self.linear_output = nn.Linear(model_dimension, model_dimension) # w_0\n",
        "\n",
        "        self.attention_weigths = None\n",
        "        self.save_weigths = save_weigths\n",
        "\n",
        "    def forward(self, queries, keys, values, mask=None):\n",
        "        projected_queries = self.linear_queries(queries) # Q * W_Q\n",
        "        projected_queries = projected_queries.view(projected_queries.shape[0], projected_queries.shape[1], self.number_of_heads, self.key_dimension).transpose(1, 2)\n",
        "\n",
        "        projected_keys = self.linear_keys(keys) # K * W_K\n",
        "        projected_keys = projected_keys.view(projected_keys.shape[0], projected_keys.shape[1], self.number_of_heads, self.key_dimension).transpose(1, 2)\n",
        "\n",
        "        projected_values = self.linear_values(values) # V * W_V\n",
        "        projected_values = projected_values.view(projected_values.shape[0], projected_values.shape[1], self.number_of_heads, self.value_dimension).transpose(1, 2)\n",
        "\n",
        "        scaled_dot_product = ScaledDotProductAttention(self.key_dimension, self.value_dimension)\n",
        "\n",
        "        attention, attention_weigths = scaled_dot_product(projected_queries, projected_keys, projected_values, mask)\n",
        "        attention = attention.transpose(1, 2).contiguous()\n",
        "        attention = attention.view(queries.shape[0], -1, self.number_of_heads * self.key_dimension)\n",
        "\n",
        "        if self.save_weigths :\n",
        "            self.attention_weigths = attention_weigths.detach()\n",
        "\n",
        "        return self.linear_output(attention)\n",
        "\n",
        "\n",
        "class PositionwiseFeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, model_dimension, inner_layer_dimension):\n",
        "        super().__init__()\n",
        "        self.modul_dimension = model_dimension\n",
        "        self.inner_layer_dimension = inner_layer_dimension\n",
        "\n",
        "        self.linear1 = nn.Linear(model_dimension, inner_layer_dimension) # W1 and b1\n",
        "        self.linear2 = nn.Linear(inner_layer_dimension, model_dimension) # W2 and b2\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        return self.linear2(x)\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, model_dimension, multihead_attention, feedforward_network, dropout_probability):\n",
        "        super().__init__()\n",
        "        self.model_dimension = model_dimension\n",
        "\n",
        "        self.multihead_attention = multihead_attention\n",
        "        self.feedforward_network = feedforward_network\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(model_dimension)\n",
        "        self.layernorm2 = nn.LayerNorm(model_dimension)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_probability)\n",
        "\n",
        "    def forward(self, src_embedded, src_mask=None):\n",
        "        attention_ouput = self.multihead_attention(src_embedded, src_embedded, src_embedded, src_mask)\n",
        "        sublayer_output1 = self.layernorm1(src_embedded + self.dropout(attention_ouput))\n",
        "\n",
        "        ffnetwork_output = self.feedforward_network(sublayer_output1)\n",
        "        sublayer_output2 = self.layernorm2(sublayer_output1 + self.dropout(ffnetwork_output))\n",
        "\n",
        "        return sublayer_output2\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, model_dimension, number_of_layers, number_of_heads, inner_layer_dimension, dropout_probability):\n",
        "        super().__init__()\n",
        "        self.model_dimension = model_dimension\n",
        "        self.number_of_layers = number_of_layers\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(\n",
        "                model_dimension,\n",
        "                MultiHeadAttention(model_dimension, number_of_heads),\n",
        "                PositionwiseFeedForwardNetwork(model_dimension, inner_layer_dimension),\n",
        "                dropout_probability\n",
        "            )\n",
        "            for _ in range(number_of_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, src_embedded, src_mask=None):\n",
        "        src_encoder_output = src_embedded\n",
        "\n",
        "        for layer in self.layers:\n",
        "            src_encoder_output = layer(src_encoder_output, src_mask)\n",
        "\n",
        "        return src_encoder_output\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, model_dimension, masked_multihead_attention, multihead_attention, feedforward_network, dropout_probability):\n",
        "        super().__init__()\n",
        "        self.model_dimension = model_dimension\n",
        "\n",
        "        self.masked_multihead_attention = masked_multihead_attention\n",
        "        self.multihead_attention = multihead_attention\n",
        "        self.feedforward_network = feedforward_network\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(model_dimension)\n",
        "        self.layernorm2 = nn.LayerNorm(model_dimension)\n",
        "        self.layernorm3 = nn.LayerNorm(model_dimension)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_probability)\n",
        "\n",
        "    def forward(self, src_encoder_output, trg_embedded, src_mask=None, trg_mask=None):\n",
        "        masked_attention_output = self.masked_multihead_attention(queries=trg_embedded, keys=trg_embedded, values=trg_embedded, mask=trg_mask)\n",
        "        sublayer_output1 = self.layernorm1(trg_embedded + self.dropout(masked_attention_output))\n",
        "\n",
        "        attention_ouput = self.multihead_attention(queries=sublayer_output1, keys=src_encoder_output, values=src_encoder_output, mask=src_mask)\n",
        "        sublayer_output2 = self.layernorm2(sublayer_output1 + self.dropout(attention_ouput))\n",
        "\n",
        "        ffnetwork_ouput = self.feedforward_network(sublayer_output2)\n",
        "        sublayer_output3 = self.layernorm3(sublayer_output2 + self.dropout(ffnetwork_ouput))\n",
        "\n",
        "        return sublayer_output3\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, model_dimension, number_of_layers, number_of_heads, inner_layer_dimension, dropout_probability):\n",
        "        super().__init__()\n",
        "        self.model_dimension = model_dimension\n",
        "        self.number_of_layers = number_of_layers\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(\n",
        "                model_dimension,\n",
        "                MultiHeadAttention(model_dimension, number_of_heads),  # masked self-attention\n",
        "                MultiHeadAttention(model_dimension, number_of_heads),  # encoder-decoder attention\n",
        "                PositionwiseFeedForwardNetwork(model_dimension, inner_layer_dimension),\n",
        "                dropout_probability\n",
        "            )\n",
        "            for _ in range(number_of_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, src_encoder_output, trg_embedded, src_mask=None, trg_mask=None):\n",
        "        trg_decoder_output = trg_embedded\n",
        "\n",
        "        for layer in self.layers:\n",
        "            trg_decoder_output = layer(src_encoder_output, trg_decoder_output, src_mask=src_mask, trg_mask=trg_mask)\n",
        "\n",
        "        return trg_decoder_output\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, model_dimension, inner_layer_dimension, number_of_layers, number_of_heads, src_vocabulary_size, trg_vocabulary_size, dropout_probability):\n",
        "        super().__init__()\n",
        "        self.model_dimension = model_dimension\n",
        "        self.number_of_layers = number_of_layers\n",
        "        self.number_of_heads = number_of_heads\n",
        "        self.src_vocabulary_size = src_vocabulary_size\n",
        "        self.trg_vocabulary_size = trg_vocabulary_size\n",
        "\n",
        "        self.input_embedding = Embedding(src_vocabulary_size, model_dimension)\n",
        "        self.output_embedding = Embedding(trg_vocabulary_size, model_dimension)\n",
        "\n",
        "        self.input_pos_encoding = PositionalEncoding(model_dimension, dropout_probability)\n",
        "        self.output_pos_encoding = PositionalEncoding(model_dimension, dropout_probability)\n",
        "\n",
        "        self.encoder = Encoder(model_dimension, number_of_layers, number_of_heads, inner_layer_dimension, dropout_probability)\n",
        "        self.decoder = Decoder(model_dimension, number_of_layers, number_of_heads, inner_layer_dimension, dropout_probability)\n",
        "\n",
        "        self.linear_projection = nn.Linear(model_dimension, trg_vocabulary_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def encode(self, src_token_ids, src_mask=None):\n",
        "        input_embedded = self.input_embedding(src_token_ids)\n",
        "        input_pos_encoded = self.input_pos_encoding(input_embedded)\n",
        "        input_encoded = self.encoder(input_pos_encoded, src_mask)\n",
        "        return input_encoded\n",
        "\n",
        "    def decode(self, input_encoded, trg_token_ids, src_mask, trg_mask=None):\n",
        "        output_embedded = self.output_embedding(trg_token_ids)\n",
        "        output_pos_encoded = self.output_pos_encoding(output_embedded)\n",
        "        output_decoded = self.decoder(input_encoded, output_pos_encoded, src_mask, trg_mask)\n",
        "        return output_decoded\n",
        "\n",
        "    def forward(self, src_token_ids, trg_token_ids, src_mask=None, trg_mask=None):\n",
        "        input_embedded = self.input_embedding(src_token_ids)\n",
        "        input_pos_encoded = self.input_pos_encoding(input_embedded)\n",
        "        input_encoded = self.encoder(input_pos_encoded, src_mask)\n",
        "\n",
        "        output_embedded = self.output_embedding(trg_token_ids)\n",
        "        output_pos_encoded = self.output_pos_encoding(output_embedded)\n",
        "        output_decoded = self.decoder(input_encoded, output_pos_encoded, src_mask, trg_mask)\n",
        "\n",
        "        output_decoded = self.linear_projection(output_decoded)\n",
        "        # output_decoded = output_decoded.softmax(dim=-1)\n",
        "\n",
        "        return output_decoded"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T13:30:19.875942Z",
          "iopub.execute_input": "2025-08-08T13:30:19.876202Z",
          "iopub.status.idle": "2025-08-08T13:30:19.901792Z",
          "shell.execute_reply.started": "2025-08-08T13:30:19.876184Z",
          "shell.execute_reply": "2025-08-08T13:30:19.901098Z"
        },
        "id": "mPgmPAI45A5n"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "class EnglishToFrenchDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, src_tokenizer, trg_tokenizer, src_language, trg_language, sequence_length):\n",
        "        super().__init__()\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.trg_tokenizer = trg_tokenizer\n",
        "        self.src_language = src_language\n",
        "        self.trg_language = trg_language\n",
        "\n",
        "        self.sos_token = torch.tensor([trg_tokenizer.token_to_id(SOS_TOKEN)], dtype=torch.int64)\n",
        "        self.eos_token = torch.tensor([trg_tokenizer.token_to_id(EOS_TOKEN)], dtype=torch.int64)\n",
        "        self.pad_token = torch.tensor([trg_tokenizer.token_to_id(PAD_TOKEN)], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_target_pair = self.dataset[idx]\n",
        "        src_text = src_target_pair['translation'][self.src_language]\n",
        "        trg_text = src_target_pair['translation'][self.trg_language]\n",
        "\n",
        "        # Transform the text into tokens\n",
        "        enc_input_tokens = self.src_tokenizer.encode(src_text).ids\n",
        "        dec_input_tokens = self.trg_tokenizer.encode(trg_text).ids\n",
        "\n",
        "        max_token_length = self.sequence_length - 2\n",
        "        if len(enc_input_tokens) > max_token_length:\n",
        "            enc_input_tokens = enc_input_tokens[:max_token_length]\n",
        "        if len(dec_input_tokens) > max_token_length - 1:  # -1 car decoder a seulement SOS au début\n",
        "            dec_input_tokens = dec_input_tokens[:max_token_length - 1]\n",
        "\n",
        "        # Add sos, eos and padding to each sentence\n",
        "        enc_num_padding_tokens = self.sequence_length - len(enc_input_tokens) - 2  # We will add <s> and </s>\n",
        "        # We will only add <s>, and </s> only on the label\n",
        "        dec_num_padding_tokens = self.sequence_length - len(dec_input_tokens) - 1\n",
        "\n",
        "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
        "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
        "            raise ValueError(\"Sentence is too long\")\n",
        "\n",
        "        # Add <s> and </s> token\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Add only <s> token\n",
        "        decoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Add only </s> token\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Double check the size of the tensors to make sure they are all sequence_length long\n",
        "        assert encoder_input.size(0) == self.sequence_length\n",
        "        assert decoder_input.size(0) == self.sequence_length\n",
        "        assert label.size(0) == self.sequence_length\n",
        "\n",
        "        encoder_mask = (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0)\n",
        "        decoder_mask = (decoder_input != self.pad_token).unsqueeze(0) & causal_mask(decoder_input.size(0))\n",
        "\n",
        "        return {\n",
        "            \"encoder_input\": encoder_input,  # (sequence_length)\n",
        "            \"decoder_input\": decoder_input,  # (sequence_length)\n",
        "            \"encoder_mask\": encoder_mask, # (1, 1, sequence_length)\n",
        "            \"decoder_mask\": decoder_mask, # (1, sequence_length) & (1, sequence_length, sequence_length),\n",
        "            \"label\": label,  # (sequence_length)\n",
        "            \"src_text\": src_text,\n",
        "            \"trg_text\": trg_text,\n",
        "        }\n",
        "\n",
        "def causal_mask(size):\n",
        "    mask = torch.triu(torch.ones((size, size)), diagonal=1).type(torch.int)\n",
        "    return (mask == 0).unsqueeze(0)\n",
        "\n",
        "\n",
        "def get_all_sentences(dataset, language):\n",
        "    for item in dataset:\n",
        "        yield item[\"translation\"][language]\n",
        "\n",
        "\n",
        "def get_tokenizer(dataset, language):\n",
        "    tokenizer_path = Path(\"tokenizer_{0}.json\".format(language))\n",
        "    if not Path.exists(tokenizer_path):\n",
        "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token=UNK_TOKEN))\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = WordLevelTrainer(special_tokens=[UNK_TOKEN, PAD_TOKEN, SOS_TOKEN, EOS_TOKEN], min_frequency=2)\n",
        "        tokenizer.train_from_iterator(get_all_sentences(dataset, language), trainer=trainer)\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "def get_dataset():\n",
        "    dataset = load_dataset(DATASET_NAME, SOURCE_LANGUAGE + \"-\" + TARGET_LANGUAGE, split=\"train\")\n",
        "    dataset = dataset.shuffle().select(range(int(len(dataset)/4)))\n",
        "\n",
        "    src_tokenizer = get_tokenizer(dataset, SOURCE_LANGUAGE)\n",
        "    trg_tokenizer = get_tokenizer(dataset, TARGET_LANGUAGE)\n",
        "\n",
        "    max_length = SEQUENCE_LENGTH - 2\n",
        "\n",
        "    def is_valid_length(example):\n",
        "        src_tokens = src_tokenizer.encode(example['translation'][SOURCE_LANGUAGE]).ids\n",
        "        trg_tokens = trg_tokenizer.encode(example['translation'][TARGET_LANGUAGE]).ids\n",
        "        return len(src_tokens) <= max_length and len(trg_tokens) <= max_length\n",
        "\n",
        "    dataset = dataset.filter(is_valid_length)\n",
        "    print(f\"Dataset filtred: {len(dataset)} examples left\")\n",
        "\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    validation_size = len(dataset) - train_size\n",
        "\n",
        "    train_dataset, valisation_dataset = random_split(dataset, [train_size, validation_size])\n",
        "\n",
        "    train_dataset = EnglishToFrenchDataset(train_dataset, src_tokenizer, trg_tokenizer, SOURCE_LANGUAGE, TARGET_LANGUAGE, SEQUENCE_LENGTH)\n",
        "    validation_dataset = EnglishToFrenchDataset(valisation_dataset, src_tokenizer, trg_tokenizer, SOURCE_LANGUAGE, TARGET_LANGUAGE, SEQUENCE_LENGTH)\n",
        "\n",
        "    # Find the maximum length of each sentence in the source and target sentence\n",
        "    src_max_length = 0\n",
        "    trg_max_length = 0\n",
        "\n",
        "    for item in dataset:\n",
        "        src_ids = src_tokenizer.encode(item['translation'][SOURCE_LANGUAGE]).ids\n",
        "        trg_ids = trg_tokenizer.encode(item['translation'][TARGET_LANGUAGE]).ids\n",
        "        src_max_length = max(src_max_length, len(src_ids))\n",
        "        trg_max_length = max(trg_max_length, len(trg_ids))\n",
        "\n",
        "    print(f'Max length of source sentence: {src_max_length}')\n",
        "    print(f'Max length of target sentence: {trg_max_length}')\n",
        "\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=True) # Process the sentences one by one\n",
        "\n",
        "    return train_dataloader, val_dataloader, src_tokenizer, trg_tokenizer"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T13:30:19.921178Z",
          "iopub.execute_input": "2025-08-08T13:30:19.921430Z",
          "iopub.status.idle": "2025-08-08T13:30:19.940233Z",
          "shell.execute_reply.started": "2025-08-08T13:30:19.921408Z",
          "shell.execute_reply": "2025-08-08T13:30:19.939577Z"
        },
        "id": "iDFokEvf5A5u"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transformer(src_vocab_size, trg_vocab_size):\n",
        "    transformer = Transformer(model_dimension=MODEL_DIMENSION,\n",
        "                              inner_layer_dimension=MODEL_INNER_LAYER_DIMENSION,\n",
        "                              number_of_layers=MODEL_NUMBER_OF_LAYERS,\n",
        "                              number_of_heads=MODEL_NUMBER_OF_HEADS,\n",
        "                              src_vocabulary_size=src_vocab_size,\n",
        "                              trg_vocabulary_size=trg_vocab_size,\n",
        "                              dropout_probability=MODEL_DROPOUT_PROBABILITY\n",
        "                            )\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def greedy_decode(model, source, source_mask, src_tokenizer, trg_tokenizer, max_len, device):\n",
        "    sos_idx = trg_tokenizer.token_to_id('[SOS]')\n",
        "    eos_idx = trg_tokenizer.token_to_id('[EOS]')\n",
        "\n",
        "    # Precompute the encoder output and reuse it for every step\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "    # Initialize the decoder input with the sos token\n",
        "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
        "\n",
        "    while True:\n",
        "        if decoder_input.size(1) == max_len:\n",
        "            break\n",
        "\n",
        "        # build mask for target\n",
        "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
        "\n",
        "        # calculate output\n",
        "        out = model.decode(encoder_output, decoder_input, source_mask, decoder_mask)\n",
        "\n",
        "        # get next token\n",
        "        prob = model.linear_projection(out[:, -1])\n",
        "        prob = model.softmax(prob)\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        decoder_input = torch.cat(\n",
        "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
        "        )\n",
        "\n",
        "        if next_word == eos_idx:\n",
        "            break\n",
        "\n",
        "    return decoder_input.squeeze(0)\n",
        "\n",
        "\n",
        "def run_validation(model, validation_ds, src_tokenizer, trg_tokenizer, max_len, device, print_msg, global_step, writer, num_examples=2):\n",
        "    model.eval()\n",
        "    count = 0\n",
        "\n",
        "    source_texts = []\n",
        "    expected = []\n",
        "    predicted = []\n",
        "\n",
        "    try:\n",
        "        # get the console window width\n",
        "        with os.popen('stty size', 'r') as console:\n",
        "            _, console_width = console.read().split()\n",
        "            console_width = int(console_width)\n",
        "    except:\n",
        "        # If we can't get the console width, use 80 as default\n",
        "        console_width = 80\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_ds:\n",
        "            count += 1\n",
        "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
        "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
        "\n",
        "            # check that the batch size is 1\n",
        "            assert encoder_input.size(\n",
        "                0) == 1, \"Batch size must be 1 for validation\"\n",
        "\n",
        "            model_out = greedy_decode(model, encoder_input, encoder_mask, src_tokenizer, trg_tokenizer, max_len, device)\n",
        "\n",
        "            source_text = batch[\"src_text\"][0]\n",
        "            target_text = batch[\"trg_text\"][0]\n",
        "            model_out_text = trg_tokenizer.decode(model_out.detach().cpu().numpy())\n",
        "\n",
        "            source_texts.append(source_text)\n",
        "            expected.append(target_text)\n",
        "            predicted.append(model_out_text)\n",
        "\n",
        "            # Print the source, target and model output\n",
        "            print_msg('-'*console_width)\n",
        "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
        "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
        "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
        "\n",
        "            if count == num_examples:\n",
        "                print_msg('-'*console_width)\n",
        "                break\n",
        "\n",
        "    if writer:\n",
        "        # Evaluate the character error rate\n",
        "        # Compute the char error rate\n",
        "        metric = torchmetrics.CharErrorRate()\n",
        "        cer = metric(predicted, expected)\n",
        "        writer.add_scalar('validation cer', cer, global_step)\n",
        "        writer.flush()\n",
        "\n",
        "        # Compute the word error rate\n",
        "        metric = torchmetrics.WordErrorRate()\n",
        "        wer = metric(predicted, expected)\n",
        "        writer.add_scalar('validation wer', wer, global_step)\n",
        "        writer.flush()\n",
        "\n",
        "        # Compute the BLEU metric\n",
        "        metric = torchmetrics.BLEUScore()\n",
        "        bleu = metric(predicted, expected)\n",
        "        writer.add_scalar('validation BLEU', bleu, global_step)\n",
        "        writer.flush()\n",
        "\n",
        "\n",
        "def learning_rate(step):\n",
        "    if step == 0:\n",
        "        step = 1\n",
        "    return MODEL_DIMENSION ** (-0.5) * min(step ** (-0.5), step * WARMUP_STEPS ** (-1.5))\n",
        "\n",
        "\n",
        "def train_model():\n",
        "    # Define the device\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Using device:\", device)\n",
        "    if (device == 'cuda'):\n",
        "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
        "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
        "    device = torch.device(device)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Make sure the weights folder exists\n",
        "    Path(f\"{DATASET_NAME}_{MODEL_FOLDER}\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_dataloader, val_dataloader, src_tokenizer, trg_tokenizer = get_dataset()\n",
        "    model = get_transformer(src_tokenizer.get_vocab_size(), trg_tokenizer.get_vocab_size()).to(device)\n",
        "\n",
        "    # Tensorboard\n",
        "    writer = SummaryWriter(EXPERIMENT_FOLDER)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(BETA1, BETA2), eps=EPSILON)\n",
        "    #lr_scheduler = LambdaLR(optimizer=optimizer, lr_lambda=lambda step: learning_rate(step))\n",
        "\n",
        "    # If the user specified a model to preload before training, load it\n",
        "    initial_epoch = 1\n",
        "    global_step = 0\n",
        "    preload = MODEL_PRELOAD\n",
        "    model_filename = latest_weights_file_path() if preload == 'latest' else get_weights_file_path(preload) if preload else None\n",
        "    if model_filename:\n",
        "        print(f'Preloading model {model_filename}')\n",
        "        state = torch.load(model_filename)\n",
        "        model.load_state_dict(state['model_state_dict'])\n",
        "        initial_epoch = state['epoch'] + 1\n",
        "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        global_step = state['global_step']\n",
        "    else:\n",
        "        print('No model to preload, starting from scratch')\n",
        "\n",
        "    loss_function = nn.CrossEntropyLoss(ignore_index=trg_tokenizer.token_to_id('[PAD]'), label_smoothing=MODEL_LABEL_SMOOTHING_VALUE).to(device)\n",
        "\n",
        "    for epoch in range(initial_epoch, NUMBER_OF_EPOCHS + 1):\n",
        "        torch.cuda.empty_cache()\n",
        "        model.train()\n",
        "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
        "        for batch in batch_iterator:\n",
        "\n",
        "            encoder_input = batch['encoder_input'].to(device) # (B, seq_len)\n",
        "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
        "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
        "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
        "\n",
        "            # Run the tensors through the encoder, decoder and the projection layer\n",
        "            model_output = model(encoder_input, decoder_input, encoder_mask, decoder_mask)\n",
        "\n",
        "            # Compare the output with the label\n",
        "            label = batch['label'].to(device) # (B, seq_len)\n",
        "\n",
        "            # Compute the loss using a simple cross entropy\n",
        "            loss = loss_function(model_output.view(-1, trg_tokenizer.get_vocab_size()), label.view(-1))\n",
        "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "            # Log the loss\n",
        "            writer.add_scalar('train loss', loss.item(), global_step)\n",
        "            writer.flush()\n",
        "\n",
        "            # Backpropagate the loss\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            # Update the weights\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            #lr_scheduler.step()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        # Run validation at the end of every epoch\n",
        "        run_validation(model, val_dataloader, src_tokenizer, trg_tokenizer, SEQUENCE_LENGTH, device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
        "\n",
        "        # Save the model at the end of every epoch\n",
        "        model_filename = get_weights_file_path(f\"{epoch:02d}\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'global_step': global_step\n",
        "        }, model_filename)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T13:30:19.940915Z",
          "iopub.execute_input": "2025-08-08T13:30:19.941110Z",
          "iopub.status.idle": "2025-08-08T13:30:19.960133Z",
          "shell.execute_reply.started": "2025-08-08T13:30:19.941095Z",
          "shell.execute_reply": "2025-08-08T13:30:19.959450Z"
        },
        "id": "E54tYDlh5A5w"
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798,
          "referenced_widgets": [
            "cf0fc630575d4182a51cc86f6a384d5d",
            "28d29988957d4a7db4805dd5964cc289",
            "7c56421dd1174ae1b2e93b880bd4fcbd",
            "576fa42f771a42749a77c4c1627329be",
            "d6b61b8fc7d2410288610e76a59e2390",
            "b0be4f546fe448438a1b9e0621c675d0",
            "e413d765a2484e1f889608356a39098f",
            "f5ec49313a3647a2af2905441160ecf8",
            "5d9d5b11e36249bcad85d5413a44b8f2",
            "d6f0e4e720064572acc3966a3b468aa0",
            "8b17b678082642e9937a62a5f0ab5330"
          ]
        },
        "id": "M0jId-UHK86g",
        "outputId": "bc51c19b-ac99-4784-bd31-3e83a77b4dd8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Device name: Tesla T4\n",
            "Device memory: 14.74127197265625 GB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/31771 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf0fc630575d4182a51cc86f6a384d5d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset filtred: 31771 examples left\n",
            "Max length of source sentence: 308\n",
            "Max length of target sentence: 297\n",
            "No model to preload, starting from scratch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 01: 100%|██████████| 3177/3177 [07:37<00:00,  6.95it/s, loss=5.163]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: And she rushed out of the room.\n",
            "    TARGET: Et elle s'élança hors de l'appartement.\n",
            " PREDICTED: Et elle se à la porte .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"Done me good!\" I exclaimed.\n",
            "    TARGET: --Calmé!» m'écriai-je.\n",
            " PREDICTED: -- Je me dis - je .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 02: 100%|██████████| 3177/3177 [07:38<00:00,  6.93it/s, loss=5.081]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Le bon archevêque entrait dans des détails infinis, comme on peut en juger par ceux que nous venons de rapporter.\n",
            "    TARGET: The good Archbishop went into endless details, as may be judged by those we have extracted from his letter.\n",
            " PREDICTED: \" It is not a , and the Duchessa of the Duchessa , and the Duchessa of the Duchessa of the Duchessa .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Accordingly, people have proposed naming this devilfish Bouguer's Squid.\"\n",
            "    TARGET: Aussi a-t-on proposé de nommer ce poulpe « calmar de Bouguer ».\n",
            " PREDICTED: Le lendemain , c ' est ce qui a fait ce qui a fait ce qui a fait ce qui a fait ce qui a fait ce qui a fait ce qui a fait ce qui a fait ce qui a fait ce qui a fait .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 03:   8%|▊         | 248/3177 [00:35<07:02,  6.93it/s, loss=5.321]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-729386109.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2628560953.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# Compare the output with the label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;31m# Compute the loss using a simple cross entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_overfitting():\n",
        "    \"\"\"Test si le modèle peut overfit sur un seul batch (il devrait le faire facilement)\"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Using device:\", device)\n",
        "    train_dataloader, val_dataloader, src_tokenizer, trg_tokenizer = get_dataset()\n",
        "    model = get_transformer(src_tokenizer.get_vocab_size(), trg_tokenizer.get_vocab_size()).to(device)\n",
        "    # Prenez 1 seul batch\n",
        "    single_batch = next(iter(train_dataloader))\n",
        "\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    print(\"Testing overfitting on single batch...\")\n",
        "    for step in range(100):\n",
        "        encoder_input = single_batch['encoder_input'].to(device)\n",
        "        decoder_input = single_batch['decoder_input'].to(device)\n",
        "        encoder_mask = single_batch['encoder_mask'].to(device)\n",
        "        decoder_mask = single_batch['decoder_mask'].to(device)\n",
        "        label = single_batch['label'].to(device)\n",
        "\n",
        "        model_output = model(encoder_input, decoder_input, encoder_mask, decoder_mask)\n",
        "        loss = nn.CrossEntropyLoss(ignore_index=trg_tokenizer.token_to_id('[PAD]'))(\n",
        "            model_output.view(-1, trg_tokenizer.get_vocab_size()),\n",
        "            label.view(-1)\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if step % 20 == 0:\n",
        "            print(f\"Step {step}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "    if loss.item() < 0.1:\n",
        "        print(\"Model CAN overfit - the architecture is working\")\n",
        "    else:\n",
        "        print(\"Model CANNOT overfit - there's a fundamental problem\")\n",
        "\n",
        "test_overfitting()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "eb73937aea114be1a435dc4ebc91e8d3",
            "bc7eb54f006145e3b810132377760281",
            "d5781cf27aa341adb2144f27d0d7c58a",
            "30510dc7c0c64d709be206db6f23ec67",
            "a7abd66af4274eb69148f12b696ea674",
            "fcd96194a4c04dfd8964ca67de5141eb",
            "fd61d8fa3cb8494ca2944d22202c995d",
            "2774dceb9d2c4049b0dc6234f1dee215",
            "573e999c4d41471185cd860c688f6f96",
            "afb70919916a49a88ce3f8e107dbbdf2",
            "f626be653d284ff9afd85587f34d74b9"
          ]
        },
        "id": "QXgnS1OFN4Po",
        "outputId": "9d7f8240-9a00-483e-aaca-33bde84107e0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/31771 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb73937aea114be1a435dc4ebc91e8d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset filtred: 31771 examples left\n",
            "Max length of source sentence: 324\n",
            "Max length of target sentence: 363\n",
            "Testing overfitting on single batch...\n",
            "Step 0: Loss = 10.2246\n",
            "Step 20: Loss = 0.5448\n",
            "Step 40: Loss = 0.0325\n",
            "Step 60: Loss = 0.0122\n",
            "Step 80: Loss = 0.0080\n",
            "✅ Model CAN overfit - the architecture is working\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Copy entire folder to Google Drive\n",
        "source_folder = 'Helsinki-NLP'\n",
        "destination_folder = '/content/drive/MyDrive/transformer_project/Helsinki/NLP'\n",
        "\n",
        "\n",
        "if os.path.exists(source_folder):\n",
        "    shutil.copytree(source_folder, destination_folder, dirs_exist_ok=True)\n",
        "    print(f\"Copied folder {source_folder} to Google Drive\")"
      ],
      "metadata": {
        "id": "dNE7Nev2g5UZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}