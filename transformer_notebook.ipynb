{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-08T13:30:19.785643Z",
          "iopub.status.busy": "2025-08-08T13:30:19.784975Z",
          "iopub.status.idle": "2025-08-08T13:30:19.789877Z",
          "shell.execute_reply": "2025-08-08T13:30:19.789127Z",
          "shell.execute_reply.started": "2025-08-08T13:30:19.785618Z"
        },
        "id": "nwKFh_PV5A5j",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# Huggingface datasets and tokenizers\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torchmetrics\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-08T13:30:19.903171Z",
          "iopub.status.busy": "2025-08-08T13:30:19.902956Z",
          "iopub.status.idle": "2025-08-08T13:30:19.920365Z",
          "shell.execute_reply": "2025-08-08T13:30:19.919775Z",
          "shell.execute_reply.started": "2025-08-08T13:30:19.903154Z"
        },
        "id": "nGAC60AY5A5p",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Transformer model parameters\n",
        "MODEL_NUMBER_OF_LAYERS = 3\n",
        "MODEL_DIMENSION = 256\n",
        "MODEL_NUMBER_OF_HEADS = 4\n",
        "MODEL_INNER_LAYER_DIMENSION = 1024\n",
        "MODEL_DROPOUT_PROBABILITY = 0.1\n",
        "MODEL_LABEL_SMOOTHING_VALUE = 0.1\n",
        "SEQUENCE_LENGTH = 500\n",
        "\n",
        "# Training parameters\n",
        "BATCH_SIZE = 4\n",
        "NUMBER_OF_EPOCHS = 10\n",
        "BETA1 = 0.9\n",
        "BETA2 = 0.98\n",
        "EPSILON = 1e-9\n",
        "WARMUP_STEPS = 4000\n",
        "\n",
        "# Dataset parameters\n",
        "DATASET_NAME = \"Helsinki-NLP/opus_books\"\n",
        "SOURCE_LANGUAGE = \"en\"\n",
        "TARGET_LANGUAGE = \"fr\"\n",
        "\n",
        "# Saving parameters\n",
        "MODEL_FOLDER = \"weights\"\n",
        "MODEL_BASENAME = \"tmodel_\"\n",
        "MODEL_PRELOAD = \"latest\"\n",
        "EXPERIMENT_FOLDER = \"runs/tmodel\"\n",
        "\n",
        "# Special tokens\n",
        "UNK_TOKEN = '[UNK]'\n",
        "SOS_TOKEN = '[SOS]'\n",
        "EOS_TOKEN = '[EOS]'\n",
        "PAD_TOKEN = \"[PAD]\"\n",
        "\n",
        "CHECKPOINTS_PATH = os.path.join(os.getcwd(), 'models', 'checkpoints') # semi-trained models during training will be dumped here\n",
        "BINARIES_PATH = os.path.join(os.getcwd(), 'models', 'binaries') # location where trained models are located\n",
        "DATA_DIR_PATH = os.path.join(os.getcwd(), 'data') # training data will be stored here\n",
        "\n",
        "os.makedirs(CHECKPOINTS_PATH, exist_ok=True)\n",
        "os.makedirs(BINARIES_PATH, exist_ok=True)\n",
        "os.makedirs(DATA_DIR_PATH, exist_ok=True)\n",
        "\n",
        "def get_weights_file_path(epoch: str):\n",
        "    model_folder = f\"{DATASET_NAME}_{MODEL_FOLDER}\"\n",
        "    model_filename = f\"{MODEL_BASENAME}{epoch}.pt\"\n",
        "    return str(Path('.') / model_folder / model_filename)\n",
        "\n",
        "# Find the latest weights file in the weights folder\n",
        "def latest_weights_file_path():\n",
        "    model_folder = f\"{DATASET_NAME}_{MODEL_FOLDER}\"\n",
        "    model_filename = f\"{MODEL_BASENAME}*\"\n",
        "    weights_files = list(Path(model_folder).glob(model_filename))\n",
        "    if len(weights_files) == 0:\n",
        "        return None\n",
        "    weights_files.sort()\n",
        "    return str(weights_files[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-08-08T13:30:19.876202Z",
          "iopub.status.busy": "2025-08-08T13:30:19.875942Z",
          "iopub.status.idle": "2025-08-08T13:30:19.901792Z",
          "shell.execute_reply": "2025-08-08T13:30:19.901098Z",
          "shell.execute_reply.started": "2025-08-08T13:30:19.876184Z"
        },
        "id": "mPgmPAI45A5n",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocabulary_size, model_dimension):\n",
        "        super().__init__()\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        self.model_dimension = model_dimension\n",
        "        self.embedding = nn.Embedding(vocabulary_size, model_dimension)\n",
        "\n",
        "    def forward(self, src_token_ids):\n",
        "        return self.embedding(src_token_ids) * math.sqrt(self.model_dimension)\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, model_dimension, max_sequence_length=5000):\n",
        "        super().__init__()\n",
        "        self.sequence_length = max_sequence_length\n",
        "        self.model_dimension = model_dimension\n",
        "\n",
        "        positional_encoding = torch.zeros(max_sequence_length, model_dimension)\n",
        "\n",
        "        positions = torch.arange(0, max_sequence_length, 1, dtype=float)\n",
        "        positions = torch.unsqueeze(positions, 1)\n",
        "        denominator = torch.exp(torch.arange(0, model_dimension, 2, dtype=float) * -math.log(10000.) / model_dimension)\n",
        "\n",
        "        positional_encoding[:,0::2] = torch.sin(positions * denominator)\n",
        "        positional_encoding[:,1::2] = torch.cos(positions * denominator)\n",
        "\n",
        "        positional_encoding = positional_encoding.unsqueeze(0)\n",
        "        self.register_buffer(\"positional_encoding\", positional_encoding)\n",
        "\n",
        "    def forward(self, src_embedded):\n",
        "        return src_embedded + self.positional_encoding[:, :src_embedded.shape[1]]\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, key_dimension, value_dimension):\n",
        "        super().__init__()\n",
        "        self.key_dimension = key_dimension\n",
        "        self.value_dimension = value_dimension\n",
        "\n",
        "    def forward(self, queries, keys, values, mask=None):\n",
        "        dot_product = torch.matmul(queries, keys.transpose(-2, -1))\n",
        "        dot_product /= math.sqrt(self.key_dimension)\n",
        "\n",
        "        if mask is not None:\n",
        "            dot_product.masked_fill_(mask == 0, float(\"-inf\"))\n",
        "\n",
        "        weights = dot_product.softmax(dim=-1)\n",
        "\n",
        "        return torch.matmul(weights, values), weights\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, model_dimension, number_of_heads, save_weigths=False):\n",
        "        super().__init__()\n",
        "        self.model_dimension = model_dimension\n",
        "        self.number_of_heads = number_of_heads\n",
        "        self.key_dimension = model_dimension // number_of_heads\n",
        "        self.value_dimension = model_dimension // number_of_heads\n",
        "\n",
        "        self.linear_queries = nn.Linear(model_dimension, model_dimension) # W_Q\n",
        "        self.linear_keys = nn.Linear(model_dimension, model_dimension) # W_K\n",
        "        self.linear_values = nn.Linear(model_dimension, model_dimension) # W_V\n",
        "\n",
        "        self.linear_output = nn.Linear(model_dimension, model_dimension) # w_0\n",
        "\n",
        "        self.attention_weigths = None\n",
        "        self.save_weigths = save_weigths\n",
        "\n",
        "    def forward(self, queries, keys, values, mask=None):\n",
        "        projected_queries = self.linear_queries(queries) # Q * W_Q\n",
        "        projected_queries = projected_queries.view(projected_queries.shape[0], projected_queries.shape[1], self.number_of_heads, self.key_dimension).transpose(1, 2)\n",
        "\n",
        "        projected_keys = self.linear_keys(keys) # K * W_K\n",
        "        projected_keys = projected_keys.view(projected_keys.shape[0], projected_keys.shape[1], self.number_of_heads, self.key_dimension).transpose(1, 2)\n",
        "\n",
        "        projected_values = self.linear_values(values) # V * W_V\n",
        "        projected_values = projected_values.view(projected_values.shape[0], projected_values.shape[1], self.number_of_heads, self.value_dimension).transpose(1, 2)\n",
        "\n",
        "        scaled_dot_product = ScaledDotProductAttention(self.key_dimension, self.value_dimension)\n",
        "\n",
        "        attention, attention_weigths = scaled_dot_product(projected_queries, projected_keys, projected_values, mask)\n",
        "        attention = attention.transpose(1, 2).contiguous()\n",
        "        attention = attention.view(queries.shape[0], -1, self.number_of_heads * self.key_dimension)\n",
        "\n",
        "        if self.save_weigths :\n",
        "            self.attention_weigths = attention_weigths.detach()\n",
        "\n",
        "        return self.linear_output(attention)\n",
        "\n",
        "\n",
        "class PositionwiseFeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, model_dimension, inner_layer_dimension):\n",
        "        super().__init__()\n",
        "        self.modul_dimension = model_dimension\n",
        "        self.inner_layer_dimension = inner_layer_dimension\n",
        "\n",
        "        self.linear1 = nn.Linear(model_dimension, inner_layer_dimension) # W1 and b1\n",
        "        self.linear2 = nn.Linear(inner_layer_dimension, model_dimension) # W2 and b2\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        return self.linear2(x)\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, model_dimension, multihead_attention, feedforward_network):\n",
        "        super().__init__()\n",
        "        self.model_dimension = model_dimension\n",
        "\n",
        "        self.multihead_attention = multihead_attention\n",
        "        self.feedforward_network = feedforward_network\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(model_dimension)\n",
        "        self.layernorm2 = nn.LayerNorm(model_dimension)\n",
        "\n",
        "    def forward(self, src_embedded, src_mask=None):\n",
        "        attention_ouput = self.multihead_attention(src_embedded, src_embedded, src_embedded, src_mask)\n",
        "        sublayer_output1 = self.layernorm1(src_embedded + attention_ouput)\n",
        "\n",
        "        ffnetwork_output = self.feedforward_network(sublayer_output1)\n",
        "        sublayer_output2 = self.layernorm2(sublayer_output1 + ffnetwork_output)\n",
        "\n",
        "        return sublayer_output2\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, model_dimension, number_of_layers, number_of_heads, inner_layer_dimension):\n",
        "        super().__init__()\n",
        "        self.model_dimension = model_dimension\n",
        "        self.number_of_layers = number_of_layers\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(\n",
        "                model_dimension,\n",
        "                MultiHeadAttention(model_dimension, number_of_heads),\n",
        "                PositionwiseFeedForwardNetwork(model_dimension, inner_layer_dimension)\n",
        "            )\n",
        "            for _ in range(number_of_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, src_embedded, src_mask=None):\n",
        "        src_encoder_output = src_embedded\n",
        "\n",
        "        for layer in self.layers:\n",
        "            src_encoder_output = layer(src_encoder_output, src_mask)\n",
        "\n",
        "        return src_encoder_output\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, model_dimension, masked_multihead_attention, multihead_attention, feedforward_network):\n",
        "        super().__init__()\n",
        "        self.model_dimension = model_dimension\n",
        "\n",
        "        self.masked_multihead_attention = masked_multihead_attention\n",
        "        self.multihead_attention = multihead_attention\n",
        "        self.feedforward_network = feedforward_network\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(model_dimension)\n",
        "        self.layernorm2 = nn.LayerNorm(model_dimension)\n",
        "        self.layernorm3 = nn.LayerNorm(model_dimension)\n",
        "\n",
        "    def forward(self, src_encoder_output, trg_embedded, src_mask=None, trg_mask=None):\n",
        "        masked_attention_output = self.masked_multihead_attention(queries=trg_embedded, keys=trg_embedded, values=trg_embedded, mask=trg_mask)\n",
        "        sublayer_output1 = self.layernorm1(trg_embedded + masked_attention_output)\n",
        "\n",
        "        attention_ouput = self.multihead_attention(queries=sublayer_output1, keys=src_encoder_output, values=src_encoder_output, mask=src_mask)\n",
        "        sublayer_output2 = self.layernorm2(sublayer_output1 + attention_ouput)\n",
        "\n",
        "        ffnetwork_ouput = self.feedforward_network(sublayer_output2)\n",
        "        sublayer_output3 = self.layernorm3(sublayer_output2 + ffnetwork_ouput)\n",
        "\n",
        "        return sublayer_output3\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, model_dimension, number_of_layers, number_of_heads, inner_layer_dimension):\n",
        "        super().__init__()\n",
        "        self.model_dimension = model_dimension\n",
        "        self.number_of_layers = number_of_layers\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(\n",
        "                model_dimension,\n",
        "                MultiHeadAttention(model_dimension, number_of_heads),  # masked self-attention\n",
        "                MultiHeadAttention(model_dimension, number_of_heads),  # encoder-decoder attention\n",
        "                PositionwiseFeedForwardNetwork(model_dimension, inner_layer_dimension)\n",
        "            )\n",
        "            for _ in range(number_of_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, src_encoder_output, trg_embedded, src_mask=None, trg_mask=None):\n",
        "        trg_decoder_output = trg_embedded\n",
        "\n",
        "        for layer in self.layers:\n",
        "            trg_decoder_output = layer(src_encoder_output, trg_decoder_output, src_mask=src_mask, trg_mask=trg_mask)\n",
        "\n",
        "        return trg_decoder_output\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, model_dimension, inner_layer_dimension, number_of_layers, number_of_heads, src_vocabulary_size, trg_vocabulary_size):\n",
        "        super().__init__()\n",
        "        self.model_dimension = model_dimension\n",
        "        self.number_of_layers = number_of_layers\n",
        "        self.number_of_heads = number_of_heads\n",
        "        self.src_vocabulary_size = src_vocabulary_size\n",
        "        self.trg_vocabulary_size = trg_vocabulary_size\n",
        "\n",
        "        self.input_embedding = Embedding(src_vocabulary_size, model_dimension)\n",
        "        self.output_embedding = Embedding(trg_vocabulary_size, model_dimension)\n",
        "\n",
        "        self.input_pos_encoding = PositionalEncoding(model_dimension)\n",
        "        self.output_pos_encoding = PositionalEncoding(model_dimension)\n",
        "\n",
        "        self.encoder = Encoder(model_dimension, number_of_layers, number_of_heads, inner_layer_dimension)\n",
        "        self.decoder = Decoder(model_dimension, number_of_layers, number_of_heads, inner_layer_dimension)\n",
        "\n",
        "        self.linear_projection = nn.Linear(model_dimension, trg_vocabulary_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def encode(self, src_token_ids, src_mask=None):\n",
        "        input_embedded = self.input_embedding(src_token_ids)\n",
        "        input_pos_encoded = self.input_pos_encoding(input_embedded)\n",
        "        input_encoded = self.encoder(input_pos_encoded, src_mask)\n",
        "        return input_encoded\n",
        "\n",
        "    def decode(self, input_encoded, trg_token_ids, src_mask, trg_mask=None):\n",
        "        output_embedded = self.output_embedding(trg_token_ids)\n",
        "        output_pos_encoded = self.output_pos_encoding(output_embedded)\n",
        "        output_decoded = self.decoder(input_encoded, output_pos_encoded, src_mask, trg_mask)\n",
        "        return output_decoded\n",
        "\n",
        "    def forward(self, src_token_ids, trg_token_ids, src_mask=None, trg_mask=None):\n",
        "        input_embedded = self.input_embedding(src_token_ids)\n",
        "        input_pos_encoded = self.input_pos_encoding(input_embedded)\n",
        "        input_encoded = self.encoder(input_pos_encoded, src_mask)\n",
        "\n",
        "        output_embedded = self.output_embedding(trg_token_ids)\n",
        "        output_pos_encoded = self.output_pos_encoding(output_embedded)\n",
        "        output_decoded = self.decoder(input_encoded, output_pos_encoded, src_mask, trg_mask)\n",
        "\n",
        "        output_decoded = self.linear_projection(output_decoded)\n",
        "        # output_decoded = output_decoded.softmax(dim=-1)\n",
        "\n",
        "        return output_decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-08T13:30:19.921430Z",
          "iopub.status.busy": "2025-08-08T13:30:19.921178Z",
          "iopub.status.idle": "2025-08-08T13:30:19.940233Z",
          "shell.execute_reply": "2025-08-08T13:30:19.939577Z",
          "shell.execute_reply.started": "2025-08-08T13:30:19.921408Z"
        },
        "id": "iDFokEvf5A5u",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class EnglishToFrenchDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, src_tokenizer, trg_tokenizer, src_language, trg_language, sequence_length):\n",
        "        super().__init__()\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.trg_tokenizer = trg_tokenizer\n",
        "        self.src_language = src_language\n",
        "        self.trg_language = trg_language\n",
        "\n",
        "        self.sos_token = torch.tensor([trg_tokenizer.token_to_id(SOS_TOKEN)], dtype=torch.int64)\n",
        "        self.eos_token = torch.tensor([trg_tokenizer.token_to_id(EOS_TOKEN)], dtype=torch.int64)\n",
        "        self.pad_token = torch.tensor([trg_tokenizer.token_to_id(PAD_TOKEN)], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_target_pair = self.dataset[idx]\n",
        "        src_text = src_target_pair['translation'][self.src_language]\n",
        "        trg_text = src_target_pair['translation'][self.trg_language]\n",
        "\n",
        "        # Transform the text into tokens\n",
        "        enc_input_tokens = self.src_tokenizer.encode(src_text).ids\n",
        "        dec_input_tokens = self.trg_tokenizer.encode(trg_text).ids\n",
        "\n",
        "        max_token_length = self.sequence_length - 2\n",
        "        if len(enc_input_tokens) > max_token_length:\n",
        "            enc_input_tokens = enc_input_tokens[:max_token_length]\n",
        "        if len(dec_input_tokens) > max_token_length - 1:  # -1 car decoder a seulement SOS au début\n",
        "            dec_input_tokens = dec_input_tokens[:max_token_length - 1]\n",
        "\n",
        "        # Add sos, eos and padding to each sentence\n",
        "        enc_num_padding_tokens = self.sequence_length - len(enc_input_tokens) - 2  # We will add <s> and </s>\n",
        "        # We will only add <s>, and </s> only on the label\n",
        "        dec_num_padding_tokens = self.sequence_length - len(dec_input_tokens) - 1\n",
        "\n",
        "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
        "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
        "            raise ValueError(\"Sentence is too long\")\n",
        "\n",
        "        # Add <s> and </s> token\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Add only <s> token\n",
        "        decoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Add only </s> token\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Double check the size of the tensors to make sure they are all sequence_length long\n",
        "        assert encoder_input.size(0) == self.sequence_length\n",
        "        assert decoder_input.size(0) == self.sequence_length\n",
        "        assert label.size(0) == self.sequence_length\n",
        "\n",
        "        encoder_mask = (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0)\n",
        "        decoder_mask = (decoder_input != self.pad_token).unsqueeze(0) & causal_mask(decoder_input.size(0))\n",
        "\n",
        "        return {\n",
        "            \"encoder_input\": encoder_input,  # (sequence_length)\n",
        "            \"decoder_input\": decoder_input,  # (sequence_length)\n",
        "            \"encoder_mask\": encoder_mask, # (1, 1, sequence_length)\n",
        "            \"decoder_mask\": decoder_mask, # (1, sequence_length) & (1, sequence_length, sequence_length),\n",
        "            \"label\": label,  # (sequence_length)\n",
        "            \"src_text\": src_text,\n",
        "            \"trg_text\": trg_text,\n",
        "        }\n",
        "\n",
        "def causal_mask(size):\n",
        "    mask = torch.triu(torch.ones((size, size)), diagonal=1).type(torch.int)\n",
        "    return (mask == 0).unsqueeze(0)\n",
        "\n",
        "\n",
        "def get_all_sentences(dataset, language):\n",
        "    for item in dataset:\n",
        "        yield item[\"translation\"][language]\n",
        "\n",
        "\n",
        "def get_tokenizer(dataset, language):\n",
        "    tokenizer_path = Path(\"tokenizer_{0}.json\".format(language))\n",
        "    if not Path.exists(tokenizer_path):\n",
        "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token=UNK_TOKEN))\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = WordLevelTrainer(special_tokens=[UNK_TOKEN, PAD_TOKEN, SOS_TOKEN, EOS_TOKEN], min_frequency=2)\n",
        "        tokenizer.train_from_iterator(get_all_sentences(dataset, language), trainer=trainer)\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "def get_dataset():\n",
        "    dataset = load_dataset(DATASET_NAME, SOURCE_LANGUAGE + \"-\" + TARGET_LANGUAGE, split=\"train\")\n",
        "    dataset = dataset.shuffle().select(range(int(len(dataset)/5)))\n",
        "\n",
        "    src_tokenizer = get_tokenizer(dataset, SOURCE_LANGUAGE)\n",
        "    trg_tokenizer = get_tokenizer(dataset, TARGET_LANGUAGE)\n",
        "\n",
        "    max_length = SEQUENCE_LENGTH - 2\n",
        "\n",
        "    def is_valid_length(example):\n",
        "        src_tokens = src_tokenizer.encode(example['translation'][SOURCE_LANGUAGE]).ids\n",
        "        trg_tokens = trg_tokenizer.encode(example['translation'][TARGET_LANGUAGE]).ids\n",
        "        return len(src_tokens) <= max_length and len(trg_tokens) <= max_length\n",
        "\n",
        "    dataset = dataset.filter(is_valid_length)\n",
        "    print(f\"Dataset filtred: {len(dataset)} examples left\")\n",
        "\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    validation_size = len(dataset) - train_size\n",
        "\n",
        "    train_dataset, valisation_dataset = random_split(dataset, [train_size, validation_size])\n",
        "\n",
        "    train_dataset = EnglishToFrenchDataset(train_dataset, src_tokenizer, trg_tokenizer, SOURCE_LANGUAGE, TARGET_LANGUAGE, SEQUENCE_LENGTH)\n",
        "    validation_dataset = EnglishToFrenchDataset(valisation_dataset, src_tokenizer, trg_tokenizer, SOURCE_LANGUAGE, TARGET_LANGUAGE, SEQUENCE_LENGTH)\n",
        "\n",
        "    # Find the maximum length of each sentence in the source and target sentence\n",
        "    src_max_length = 0\n",
        "    trg_max_length = 0\n",
        "\n",
        "    for item in dataset:\n",
        "        src_ids = src_tokenizer.encode(item['translation'][SOURCE_LANGUAGE]).ids\n",
        "        trg_ids = trg_tokenizer.encode(item['translation'][TARGET_LANGUAGE]).ids\n",
        "        src_max_length = max(src_max_length, len(src_ids))\n",
        "        trg_max_length = max(trg_max_length, len(trg_ids))\n",
        "\n",
        "    print(f'Max length of source sentence: {src_max_length}')\n",
        "    print(f'Max length of target sentence: {trg_max_length}')\n",
        "\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=True) # Process the sentences one by one\n",
        "\n",
        "    return train_dataloader, val_dataloader, src_tokenizer, trg_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-08T13:30:19.941110Z",
          "iopub.status.busy": "2025-08-08T13:30:19.940915Z",
          "iopub.status.idle": "2025-08-08T13:30:19.960133Z",
          "shell.execute_reply": "2025-08-08T13:30:19.959450Z",
          "shell.execute_reply.started": "2025-08-08T13:30:19.941095Z"
        },
        "id": "E54tYDlh5A5w",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_transformer(src_vocab_size, trg_vocab_size):\n",
        "    transformer = Transformer(model_dimension=MODEL_DIMENSION,\n",
        "                              inner_layer_dimension=MODEL_INNER_LAYER_DIMENSION,\n",
        "                              number_of_layers=MODEL_NUMBER_OF_LAYERS,\n",
        "                              number_of_heads=MODEL_NUMBER_OF_HEADS,\n",
        "                              src_vocabulary_size=src_vocab_size,\n",
        "                              trg_vocabulary_size=trg_vocab_size\n",
        "                            )\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def greedy_decode(model, source, source_mask, src_tokenizer, trg_tokenizer, max_len, device):\n",
        "    sos_idx = trg_tokenizer.token_to_id('[SOS]')\n",
        "    eos_idx = trg_tokenizer.token_to_id('[EOS]')\n",
        "\n",
        "    # Precompute the encoder output and reuse it for every step\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "    # Initialize the decoder input with the sos token\n",
        "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
        "\n",
        "    while True:\n",
        "        if decoder_input.size(1) == max_len:\n",
        "            break\n",
        "\n",
        "        # build mask for target\n",
        "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
        "\n",
        "        # calculate output\n",
        "        out = model.decode(encoder_output, decoder_input, source_mask, decoder_mask)\n",
        "\n",
        "        # get next token\n",
        "        prob = model.linear_projection(out[:, -1])\n",
        "        prob = model.softmax(prob)\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        decoder_input = torch.cat(\n",
        "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
        "        )\n",
        "\n",
        "        if next_word == eos_idx:\n",
        "            break\n",
        "\n",
        "    return decoder_input.squeeze(0)\n",
        "\n",
        "\n",
        "def run_validation(model, validation_ds, src_tokenizer, trg_tokenizer, max_len, device, print_msg, global_step, writer, num_examples=2):\n",
        "    model.eval()\n",
        "    count = 0\n",
        "\n",
        "    source_texts = []\n",
        "    expected = []\n",
        "    predicted = []\n",
        "\n",
        "    try:\n",
        "        # get the console window width\n",
        "        with os.popen('stty size', 'r') as console:\n",
        "            _, console_width = console.read().split()\n",
        "            console_width = int(console_width)\n",
        "    except:\n",
        "        # If we can't get the console width, use 80 as default\n",
        "        console_width = 80\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_ds:\n",
        "            count += 1\n",
        "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
        "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
        "\n",
        "            # check that the batch size is 1\n",
        "            assert encoder_input.size(\n",
        "                0) == 1, \"Batch size must be 1 for validation\"\n",
        "\n",
        "            model_out = greedy_decode(model, encoder_input, encoder_mask, src_tokenizer, trg_tokenizer, max_len, device)\n",
        "\n",
        "            source_text = batch[\"src_text\"][0]\n",
        "            target_text = batch[\"trg_text\"][0]\n",
        "            model_out_text = trg_tokenizer.decode(model_out.detach().cpu().numpy())\n",
        "\n",
        "            source_texts.append(source_text)\n",
        "            expected.append(target_text)\n",
        "            predicted.append(model_out_text)\n",
        "\n",
        "            # Print the source, target and model output\n",
        "            print_msg('-'*console_width)\n",
        "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
        "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
        "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
        "\n",
        "            if count == num_examples:\n",
        "                print_msg('-'*console_width)\n",
        "                break\n",
        "\n",
        "    if writer:\n",
        "        # Evaluate the character error rate\n",
        "        # Compute the char error rate\n",
        "        metric = torchmetrics.CharErrorRate()\n",
        "        cer = metric(predicted, expected)\n",
        "        writer.add_scalar('validation cer', cer, global_step)\n",
        "        writer.flush()\n",
        "\n",
        "        # Compute the word error rate\n",
        "        metric = torchmetrics.WordErrorRate()\n",
        "        wer = metric(predicted, expected)\n",
        "        writer.add_scalar('validation wer', wer, global_step)\n",
        "        writer.flush()\n",
        "\n",
        "        # Compute the BLEU metric\n",
        "        metric = torchmetrics.BLEUScore()\n",
        "        bleu = metric(predicted, expected)\n",
        "        writer.add_scalar('validation BLEU', bleu, global_step)\n",
        "        writer.flush()\n",
        "\n",
        "\n",
        "def learning_rate(step):\n",
        "    if step == 0:\n",
        "        step = 1\n",
        "    return MODEL_DIMENSION ** (-0.5) * min(step ** (-0.5), step * WARMUP_STEPS ** (-1.5))\n",
        "\n",
        "\n",
        "def train_model():\n",
        "    # Define the device\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Using device:\", device)\n",
        "    if (device == 'cuda'):\n",
        "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
        "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
        "    device = torch.device(device)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Make sure the weights folder exists\n",
        "    Path(f\"{DATASET_NAME}_{MODEL_FOLDER}\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_dataloader, val_dataloader, src_tokenizer, trg_tokenizer = get_dataset()\n",
        "    model = get_transformer(src_tokenizer.get_vocab_size(), trg_tokenizer.get_vocab_size()).to(device)\n",
        "\n",
        "    # Tensorboard\n",
        "    writer = SummaryWriter(EXPERIMENT_FOLDER)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(BETA1, BETA2), eps=EPSILON)\n",
        "    #lr_scheduler = LambdaLR(optimizer=optimizer, lr_lambda=lambda step: learning_rate(step))\n",
        "\n",
        "    # If the user specified a model to preload before training, load it\n",
        "    initial_epoch = 1\n",
        "    global_step = 0\n",
        "    preload = MODEL_PRELOAD\n",
        "    model_filename = latest_weights_file_path() if preload == 'latest' else get_weights_file_path(preload) if preload else None\n",
        "    if model_filename:\n",
        "        print(f'Preloading model {model_filename}')\n",
        "        state = torch.load(model_filename)\n",
        "        model.load_state_dict(state['model_state_dict'])\n",
        "        initial_epoch = state['epoch'] + 1\n",
        "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        global_step = state['global_step']\n",
        "    else:\n",
        "        print('No model to preload, starting from scratch')\n",
        "\n",
        "    loss_function = nn.CrossEntropyLoss(ignore_index=trg_tokenizer.token_to_id('[PAD]'), label_smoothing=MODEL_LABEL_SMOOTHING_VALUE).to(device)\n",
        "\n",
        "    for epoch in range(initial_epoch, NUMBER_OF_EPOCHS + 1):\n",
        "        torch.cuda.empty_cache()\n",
        "        model.train()\n",
        "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
        "        for batch in batch_iterator:\n",
        "\n",
        "            encoder_input = batch['encoder_input'].to(device) # (B, seq_len)\n",
        "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
        "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
        "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
        "\n",
        "            # Run the tensors through the encoder, decoder and the projection layer\n",
        "            model_output = model(encoder_input, decoder_input, encoder_mask, decoder_mask)\n",
        "\n",
        "            # Compare the output with the label\n",
        "            label = batch['label'].to(device) # (B, seq_len)\n",
        "\n",
        "            # Compute the loss using a simple cross entropy\n",
        "            loss = loss_function(model_output.view(-1, trg_tokenizer.get_vocab_size()), label.view(-1))\n",
        "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "            # Log the loss\n",
        "            writer.add_scalar('train loss', loss.item(), global_step)\n",
        "            writer.flush()\n",
        "\n",
        "            # Backpropagate the loss\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            # Update the weights\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            #lr_scheduler.step()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        # Run validation at the end of every epoch\n",
        "        run_validation(model, val_dataloader, src_tokenizer, trg_tokenizer, SEQUENCE_LENGTH, device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
        "\n",
        "        # Save the model at the end of every epoch\n",
        "        model_filename = get_weights_file_path(f\"{epoch:02d}\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'global_step': global_step\n",
        "        }, model_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8440108542f746bc92deebde3df6022a",
            "399de990fbf544b3add5eaf49f8462c4",
            "79fac447f65a4ee493e6568fbc194fec",
            "6e1b728a949e481da53da4bd91ddf50f",
            "fae3c6e1191342138cca2c3c843bc384",
            "26639033152e432ca861780deead5bbc",
            "b40e14bd41ff43c4bbf85553b881e958",
            "4d174688b7e04c6a9f72d0f8746d4e8c",
            "370f2690312b4d64aff820ac818f707b",
            "ca9d6e01367143149606846afe6de3e6",
            "9ec5ede30e274d3cadd7fb8bc8fbc2a5"
          ]
        },
        "id": "M0jId-UHK86g",
        "outputId": "17ce8fb6-60af-4bea-896f-b7466927c73e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Device name: Tesla T4\n",
            "Device memory: 14.74127197265625 GB\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8440108542f746bc92deebde3df6022a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/25417 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset filtred: 25417 examples left\n",
            "Max length of source sentence: 355\n",
            "Max length of target sentence: 363\n",
            "No model to preload, starting from scratch\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 01: 100%|██████████| 5084/5084 [07:55<00:00, 10.68it/s, loss=4.341]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"Then you use me, and yet do not trust me!\" I cried with some bitterness.\n",
            "    TARGET: – Ainsi vous vous servez de moi, et pourtant vous ne vous fiez pas à moi ! m’écriai-je avec amertume.\n",
            " PREDICTED: Alors je ne me me me me me dit que vous me faire , et je ne me pas .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Anne of Austria lowered her head, allowed the torrent to flow on without replying, hoping that it would cease of itself; but this was not what Louis XIII meant. Louis XIII wanted a discussion from which some light or other might break, convinced as he was that the cardinal had some afterthought and was preparing for him one of those terrible surprises which his Eminence was so skillful in getting up.\n",
            "    TARGET: Anne d'Autriche baissa la tête, laissa s'écouler le torrent sans répondre et espérant qu'il finirait par s'arrêter; mais ce n'était pas cela que voulait Louis XIII; Louis XIII voulait une discussion de laquelle jaillît une lumière quelconque, convaincu qu'il était que le cardinal avait quelque arrière-pensée et lui machinait une surprise terrible comme en savait faire Son Éminence.\n",
            " PREDICTED: C ' était un peu , mais , il n ' avait pas un peu , mais , que ce qui lui avait pas un peu de ce qui lui avait une voix , mais , et qu ' il avait une voix , il avait une voix de ce qui lui avait une voix , il avait une voix de ce qui lui avait une voix .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 02: 100%|██████████| 5084/5084 [07:53<00:00, 10.75it/s, loss=6.086]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: His place was no longer in the cart, but at the head of the troop.\n",
            "    TARGET: Sa place n'était plus sur le chariot, mais en tête de la caravane.\n",
            " PREDICTED: Sa tête , mais le chemin de la tête , mais le chemin de la tête .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: From that time she had a lover of whom he was ignorant.\n",
            "    TARGET: Elle eut des lors un amant, qu'il ignora.\n",
            " PREDICTED: A ce temps , elle avait fait un temps de l ’ air d ’ une fois .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 03: 100%|██████████| 5084/5084 [07:53<00:00, 10.74it/s, loss=5.115]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: I have met with two instances lately, one I will not mention; the other is Charlotte's marriage.\n",
            "    TARGET: De l’un, je ne parlerai pas ; l’autre, c’est le mariage de Charlotte.\n",
            " PREDICTED: Je n ' ai pas deux , je n ' en ai pas deux pas un de l ' autre .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The yellow barouche had swung into the avenue, and a few moments later the weary, panting horses had pulled up behind our curricle.\n",
            "    TARGET: La barouche jaune avait enfilé l'avenue, et peu d'instants après, les chevaux harassés, essoufflés, venaient de s'arrêter derrière notre voiture.\n",
            " PREDICTED: Le soir , nous avions un peu de notre tour , et nous avions un peu à l ' air de notre tour .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 04: 100%|██████████| 5084/5084 [07:53<00:00, 10.73it/s, loss=5.514]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: He climbed the plateau at the risk of being perceived and fired at, tried to extinguish the fire which was consuming the buildings of the poultry-yard, and had struggled, though in vain, against it until the cart appeared at the edge of the wood.\n",
            "    TARGET: Il était remonté sur le plateau, au risque d'y recevoir quelque balle, il avait essayé d'éteindre l'incendie qui consumait les bâtiments de la basse- cour, et il avait lutté, mais inutilement, contre le feu, jusqu'au moment où le chariot parut sur la lisière du bois.\n",
            " PREDICTED: Il était donc , et , comme si le bois était le bois , et , il avait fait des bois , et les bois , s ' il avait fait des bois contre les bois , et les bois , qui avaient été les bois , et les bois , et les bois , s ' il avait contre les bois .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: But in spite of her precautions the nails clanked; and those enormous boots stood oppressively in the room.\n",
            "    TARGET: Mais elle eut beau y mettre des précautions, les clous sonnerent; et ces chaussures énormes resterent genantes dans la piece.\n",
            " PREDICTED: Mais , dans ces , dans la chambre , dans la chambre , dans la chambre , et les , les , les , les de sa chambre .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 05: 100%|██████████| 5084/5084 [07:52<00:00, 10.76it/s, loss=4.898]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: 'I thought as much,' he told himself bitterly, 'her love is eclipsed by the joy of receiving a King in her house.\n",
            "    TARGET: Je l’avais prévu, se disait-il avec amertume, son amour s’éclipse devant le bonheur de recevoir un roi dans sa maison.\n",
            " PREDICTED: Je lui ai dit , il se disait à la joie de joie , il lui - même , il y a un amour de joie .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: These two streams, which lower down became rivers by the absorption of several tributaries, were formed by all the springs of the mountain and thus caused the fertility of its southern part.\n",
            "    TARGET: Ces deux ruisseaux, changés plus bas en rivières par l'absorption de quelques affluents, se formaient de toutes les eaux de la montagne et déterminaient ainsi la fertilité de sa portion méridionale.\n",
            " PREDICTED: Ces deux étaient , et les deux petits de la partie du sol , et les deux cents , de la partie de la partie de la partie de la partie de la partie de la partie de la partie .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 06: 100%|██████████| 5084/5084 [07:52<00:00, 10.76it/s, loss=5.991]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Whilst he went on talking, my uncle prepared a few provisions, which I devoured eagerly, notwithstanding his advice to the contrary.\n",
            "    TARGET: Tout en parlant, mon oncle apprêtait quelques aliments que je dévorai, malgré ses recommandations.\n",
            " PREDICTED: Je me sur un oncle , il reprit son oncle , en quelques instants , et quelques instants de quelques instants de leur oncle .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"To mass, sir? What for?\n",
            "    TARGET: —A la messe, monsieur le curé, pour quoi faire?\n",
            " PREDICTED: -- Que vous donc , monsieur ?\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 07: 100%|██████████| 5084/5084 [07:51<00:00, 10.78it/s, loss=6.165]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"He didn't tell you anything else?\"\n",
            "    TARGET: -- Il ne vous a rien dit de plus ?\n",
            " PREDICTED: – Il ne te dire pas ?\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Candide embraced his sheep with transport.\n",
            "    TARGET: Candide caressait son mouton.\n",
            " PREDICTED: Candide se mit à Candide .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 08: 100%|██████████| 5084/5084 [07:53<00:00, 10.75it/s, loss=5.910]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: De temps en temps, il est vrai, il se présente un cas un peu plus compliqué.\n",
            "    TARGET: Now and again a case turns up which is a little more complex.\n",
            " PREDICTED: \" You ' t ,\" he said , \" I ' s a in the , \" I should have a .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The captain adjusted Jehan's head upon an inclined plane of cabbage−stumps, and on the very instant, the scholar fell to snoring in a magnificent bass.\n",
            "    TARGET: Le capitaine arrangea la tête de Jehan sur un plan incliné de trognons de choux, et à l’instant même l’écolier se mit à ronfler avec une basse-taille magnifique.\n",
            " PREDICTED: Le capitaine , fit un instant sur le capitaine , et tomba sur la tête de la tête , sur la tête de la tête , et se faire une tête en fit signe de se faire .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 09: 100%|██████████| 5084/5084 [07:51<00:00, 10.78it/s, loss=4.108]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The next day, as she was getting up, she saw the clerk on the Place.\n",
            "    TARGET: Le lendemain, à son réveil, elle aperçut le clerc sur la place.\n",
            " PREDICTED: Le lendemain , elle vit le lendemain , comme sur le lendemain , elle vit sur la place .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: 'Ah, my dear Julien,' said M. de Renal, laughing in the most insincere manner, 'the whole day, if you wish, the whole of tomorrow, my worthy friend.\n",
            "    TARGET: – Eh, mon cher Julien ! dit M. de Rênal en riant de l’air le plus faux, toute la journée, si vous voulez, toute celle de demain, mon bon ami.\n",
            " PREDICTED: – Ah ! mon cher ami , dit Julien en se , si vous le plus grand monde , vous , vous avez tout ce jour , vous vous , vous tout est à fait de mon cher ami .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Epoch 10: 100%|██████████| 5084/5084 [07:52<00:00, 10.77it/s, loss=5.002]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Yet in spite of all this seduction Milady might fail--for Felton was forewarned, and that against the least chance.\n",
            "    TARGET: Et cependant, malgré toute cette séduction, Milady pouvait échouer, car Felton était prévenu, et cela contre le moindre hasard.\n",
            " PREDICTED: Mais , en avant tout , Milady était le moins contre tout , et Milady se contre tout .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: By way of decoration for the apartment, hanging to a nail in the middle of the wall, whose green paint scaled off from the effects of the saltpetre, was a crayon head of Minerva in gold frame, underneath which was written in Gothic letters \"To dear Papa.\"\n",
            "    TARGET: Il y avait, pour décorer l’appartement, accrochée à un clou, au milieu du mur dont la peinture verte s’écaillait sous le salpêtre, une tête de Minerve au crayon noir, encadrée de dorure, et qui portait au bas, écrit en lettres gothiques: «À mon cher papa.»\n",
            " PREDICTED: Il s ’ était dans le milieu de la chambre dont le mur était , et une était , une de la tête , dont la tête était , à la tête en effet , à la tête , à la tête , à la tête en , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à la tête , à\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "96d6f0bbbf9e43f984e460083d8188ac",
            "dcfd82e1c3c640d69acebc1c398369de",
            "5a895eb315bc4e80aecc1e17c06ae6ff",
            "42d77ca3f39e4a5ba548089e54b6ff6e",
            "2f26d57ea3ab45b2a1d36d41b71864e7",
            "69bfac45bad04cf6aa33095aa6eed489",
            "bc0d0d4d04f14175b7eb67ed402041f0",
            "90a84d184a6d4c01b3fbd6c240878735",
            "5223e7b6a3d34618929cccc3df49b0bd",
            "3de5a8e3d44543e1b2ec55e8a020a899",
            "382d22bbdc2f43f89da5ae622de7bfb7"
          ]
        },
        "id": "QXgnS1OFN4Po",
        "outputId": "1548c57f-578a-4e98-b514-bc4c75975272"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96d6f0bbbf9e43f984e460083d8188ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/25417 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset filtred: 25417 examples left\n",
            "Max length of source sentence: 361\n",
            "Max length of target sentence: 350\n",
            "Testing overfitting on single batch...\n",
            "Step 0: Loss = 10.4606\n",
            "Step 20: Loss = 0.2373\n",
            "Step 40: Loss = 0.0143\n",
            "Step 60: Loss = 0.0067\n",
            "Step 80: Loss = 0.0050\n",
            "✅ Model CAN overfit - the architecture is working\n"
          ]
        }
      ],
      "source": [
        "def test_overfitting():\n",
        "    \"\"\"Test si le modèle peut overfit sur un seul batch (il devrait le faire facilement)\"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Using device:\", device)\n",
        "    train_dataloader, val_dataloader, src_tokenizer, trg_tokenizer = get_dataset()\n",
        "    model = get_transformer(src_tokenizer.get_vocab_size(), trg_tokenizer.get_vocab_size()).to(device)\n",
        "    # Prenez 1 seul batch\n",
        "    single_batch = next(iter(train_dataloader))\n",
        "\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    print(\"Testing overfitting on single batch...\")\n",
        "    for step in range(100):\n",
        "        encoder_input = single_batch['encoder_input'].to(device)\n",
        "        decoder_input = single_batch['decoder_input'].to(device)\n",
        "        encoder_mask = single_batch['encoder_mask'].to(device)\n",
        "        decoder_mask = single_batch['decoder_mask'].to(device)\n",
        "        label = single_batch['label'].to(device)\n",
        "\n",
        "        model_output = model(encoder_input, decoder_input, encoder_mask, decoder_mask)\n",
        "        loss = nn.CrossEntropyLoss(ignore_index=trg_tokenizer.token_to_id('[PAD]'))(\n",
        "            model_output.view(-1, trg_tokenizer.get_vocab_size()),\n",
        "            label.view(-1)\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if step % 20 == 0:\n",
        "            print(f\"Step {step}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "    if loss.item() < 0.1:\n",
        "        print(\"✅ Model CAN overfit - the architecture is working\")\n",
        "    else:\n",
        "        print(\"❌ Model CANNOT overfit - there's a fundamental problem\")\n",
        "\n",
        "test_overfitting()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31090,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "26639033152e432ca861780deead5bbc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f26d57ea3ab45b2a1d36d41b71864e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "370f2690312b4d64aff820ac818f707b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "382d22bbdc2f43f89da5ae622de7bfb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "399de990fbf544b3add5eaf49f8462c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26639033152e432ca861780deead5bbc",
            "placeholder": "​",
            "style": "IPY_MODEL_b40e14bd41ff43c4bbf85553b881e958",
            "value": "Filter: 100%"
          }
        },
        "3de5a8e3d44543e1b2ec55e8a020a899": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42d77ca3f39e4a5ba548089e54b6ff6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3de5a8e3d44543e1b2ec55e8a020a899",
            "placeholder": "​",
            "style": "IPY_MODEL_382d22bbdc2f43f89da5ae622de7bfb7",
            "value": " 25417/25417 [00:02&lt;00:00, 11164.83 examples/s]"
          }
        },
        "4d174688b7e04c6a9f72d0f8746d4e8c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5223e7b6a3d34618929cccc3df49b0bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a895eb315bc4e80aecc1e17c06ae6ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90a84d184a6d4c01b3fbd6c240878735",
            "max": 25417,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5223e7b6a3d34618929cccc3df49b0bd",
            "value": 25417
          }
        },
        "69bfac45bad04cf6aa33095aa6eed489": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e1b728a949e481da53da4bd91ddf50f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca9d6e01367143149606846afe6de3e6",
            "placeholder": "​",
            "style": "IPY_MODEL_9ec5ede30e274d3cadd7fb8bc8fbc2a5",
            "value": " 25417/25417 [00:02&lt;00:00, 11004.03 examples/s]"
          }
        },
        "79fac447f65a4ee493e6568fbc194fec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d174688b7e04c6a9f72d0f8746d4e8c",
            "max": 25417,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_370f2690312b4d64aff820ac818f707b",
            "value": 25417
          }
        },
        "8440108542f746bc92deebde3df6022a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_399de990fbf544b3add5eaf49f8462c4",
              "IPY_MODEL_79fac447f65a4ee493e6568fbc194fec",
              "IPY_MODEL_6e1b728a949e481da53da4bd91ddf50f"
            ],
            "layout": "IPY_MODEL_fae3c6e1191342138cca2c3c843bc384"
          }
        },
        "90a84d184a6d4c01b3fbd6c240878735": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96d6f0bbbf9e43f984e460083d8188ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dcfd82e1c3c640d69acebc1c398369de",
              "IPY_MODEL_5a895eb315bc4e80aecc1e17c06ae6ff",
              "IPY_MODEL_42d77ca3f39e4a5ba548089e54b6ff6e"
            ],
            "layout": "IPY_MODEL_2f26d57ea3ab45b2a1d36d41b71864e7"
          }
        },
        "9ec5ede30e274d3cadd7fb8bc8fbc2a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b40e14bd41ff43c4bbf85553b881e958": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc0d0d4d04f14175b7eb67ed402041f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca9d6e01367143149606846afe6de3e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcfd82e1c3c640d69acebc1c398369de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69bfac45bad04cf6aa33095aa6eed489",
            "placeholder": "​",
            "style": "IPY_MODEL_bc0d0d4d04f14175b7eb67ed402041f0",
            "value": "Filter: 100%"
          }
        },
        "fae3c6e1191342138cca2c3c843bc384": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
